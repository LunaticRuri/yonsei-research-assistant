import os
import json
import shutil
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ì„¤ì • (run.pyì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤)
OUTPUT_DIR = "./checkpoints"
FINAL_SAVE_PATH = os.path.join(OUTPUT_DIR, "final_model")
MODEL_NAME = "BAAI/bge-reranker-v2-m3"

def find_best_checkpoint(output_dir):
    # 1. ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ í´ë” ê²€ìƒ‰
    if not os.path.exists(output_dir):
        print(f"âŒ {output_dir} í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    if not checkpoints:
        print("âŒ ì²´í¬í¬ì¸íŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 2. ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸° (ì—¬ê¸°ì— ì „ì²´ í•™ìŠµ ê¸°ë¡ì´ ìˆìŒ)
    checkpoints.sort(key=lambda x: int(x.split("-")[1]))
    latest_checkpoint = os.path.join(output_dir, checkpoints[-1])
    state_file = os.path.join(latest_checkpoint, "trainer_state.json")

    if not os.path.exists(state_file):
        print(f"âŒ {state_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 3. trainer_state.json ì½ì–´ì„œ best_model_checkpoint í™•ì¸
    with open(state_file, "r") as f:
        state = json.load(f)
    
    best_model_path = state.get("best_model_checkpoint")
    
    if best_model_path and os.path.exists(best_model_path):
        print(f"âœ… ê¸°ë¡ìƒ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸: {best_model_path}")
        print(f"   (Best Metric: {state.get('best_metric')})")
        return best_model_path
    else:
        print("âš ï¸ Best model ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print(f"â„¹ï¸ ëŒ€ì‹  ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {latest_checkpoint}")
        return latest_checkpoint

def save_final_model():
    best_ckpt = find_best_checkpoint(OUTPUT_DIR)
    
    if best_ckpt:
        print(f"ğŸ’¾ ëª¨ë¸ ë³€í™˜ ë° ì €ì¥ ì¤‘... ({best_ckpt} -> {FINAL_SAVE_PATH})")
        
        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForSequenceClassification.from_pretrained(best_ckpt, num_labels=1)
        tokenizer = AutoTokenizer.from_pretrained(best_ckpt)
        
        # ìµœì¢… ê²½ë¡œì— ì €ì¥
        model.save_pretrained(FINAL_SAVE_PATH)
        tokenizer.save_pretrained(FINAL_SAVE_PATH)
        
        print(f"ğŸ‰ ë³µêµ¬ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ì´ ì—¬ê¸°ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {FINAL_SAVE_PATH}")
    else:
        print("âŒ ë³µêµ¬í•  ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    save_final_model()