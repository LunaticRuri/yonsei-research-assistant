import os
import json
import torch
import pandas as pd
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    Trainer, 
    TrainingArguments,
    DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
MODEL_NAME = "BAAI/bge-reranker-v2-m3"
DATA_PATH = "filtered_dataset.jsonl" # ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  ë°ì´í„°ì…‹
OUTPUT_DIR = "./checkpoints"
MAX_LENGTH = 512
BATCH_SIZE = 16 # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
NUM_EPOCHS = 1
LEARNING_RATE = 2e-5
SAVE_STEPS = 2000 # 2000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥

# ==========================================
# 2. ì²´í¬í¬ì¸íŠ¸ ê°ì§€
# ==========================================
def get_last_checkpoint(output_dir):
    if os.path.exists(output_dir):
        # checkpoint-1000, checkpoint-2000 ë“±ì˜ í´ë” ê²€ìƒ‰
        checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
        if checkpoints:
            # ìˆ«ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì„ íƒ
            checkpoints.sort(key=lambda x: int(x.split("-")[1]))
            last_checkpoint = os.path.join(output_dir, checkpoints[-1])
            print(f"ğŸ”„ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {last_checkpoint} ì—ì„œ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
            return last_checkpoint
    print("ğŸš€ ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    return None

last_checkpoint = get_last_checkpoint(OUTPUT_DIR)

# ==========================================
# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (Pointwise ë³€í™˜)
# ==========================================
# HF TrainerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ (Input, Label) ìŒì„ ì„ í˜¸í•˜ë¯€ë¡œ 
# Triplet(Q, P, N)ì„ -> (Q, P, 1) ê³¼ (Q, N, 0) ë‘ ê°œì˜ ë°ì´í„°ë¡œ ìª¼ê° ë‹¤.

print("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
data_entries = []
with open(DATA_PATH, 'r', encoding='utf-8') as f:
    for line in f:
        entry = json.loads(line)
        # Positive Sample (Label 1.0)
        data_entries.append({
            "text_a": entry['query'],
            "text_b": entry['positive'],
            "labels": 1.0
        })
        # Negative Sample (Label 0.0)
        data_entries.append({
            "text_a": entry['query'],
            "text_b": entry['negative'],
            "labels": 0.0
        })

df = pd.DataFrame(data_entries)
train_df, val_df = train_test_split(df, test_size=0.05, random_state=42)

# PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜
class RerankDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data.iloc[idx]
        # Cross-Encoder ì…ë ¥ í˜•ì‹: [CLS] Query [SEP] Document [SEP]
        tokenized = self.tokenizer(
            item['text_a'],
            item['text_b'],
            truncation=True,
            max_length=self.max_length,
            padding=False # DataCollatorê°€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ íŒ¨ë”©í•¨ (ì†ë„ í–¥ìƒ)
        )
        tokenized['labels'] = torch.tensor(item['labels'], dtype=torch.float)
        return tokenized

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
train_dataset = RerankDataset(train_df, tokenizer, MAX_LENGTH)
eval_dataset = RerankDataset(val_df, tokenizer, MAX_LENGTH)

# ==========================================
# 4. ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ì„¤ì •
# ==========================================
# num_labels=1 ì„¤ì •: íšŒê·€(Regression) ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ ì ìˆ˜(Score)ë¥¼ ì˜ˆì¸¡
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=1,
    ignore_mismatched_sizes=True 
)

# Spot Instanceì— ìµœì í™”ëœ Training Arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=False,      # ë®ì–´ì“°ê¸° ê¸ˆì§€ (ì²´í¬í¬ì¸íŠ¸ ë³´í˜¸)
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    
    # [Spot Instance í•µì‹¬ ì„¤ì •]
    save_strategy="steps",           # ìŠ¤í… ë‹¨ìœ„ë¡œ ì €ì¥
    save_steps=SAVE_STEPS,           # ì €ì¥ ê°„ê²©
    save_total_limit=3,              # ë””ìŠ¤í¬ ìš©ëŸ‰ ê´€ë¦¬ë¥¼ ìœ„í•´ ìµœê·¼ 3ê°œë§Œ ìœ ì§€
    load_best_model_at_end=True,     # í•™ìŠµ ì¢…ë£Œ ì‹œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    
    eval_strategy="steps",
    eval_steps=SAVE_STEPS,           # ì €ì¥í•  ë•Œ í‰ê°€ë„ ê°™ì´ ìˆ˜í–‰
    fp16=True,                       # GPU ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
    dataloader_num_workers=4,        # ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ
    report_to="none"                 # ë¡œê¹… ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ (Resume ë¡œì§ í¬í•¨)
# ==========================================
print("í•™ìŠµ ì‹œì‘...")

# ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œë¶€í„°, ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„°(resume_from_checkpoint=None)
trainer.train(resume_from_checkpoint=last_checkpoint)

print("í•™ìŠµ ì™„ë£Œ. ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
final_save_path = os.path.join(OUTPUT_DIR, "final_model")
trainer.save_model(final_save_path)
tokenizer.save_pretrained(final_save_path)
print(f"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê²½ë¡œ: {final_save_path}")
