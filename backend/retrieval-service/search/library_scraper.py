import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, quote
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

class LibraryScraper:
    """ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    
    def __init__(self):
        self.base_url = "https://library.yonsei.ac.kr"
        self.search_endpoints = {
            "integrated": "/search/tot",
            "books": "/search/book", 
            "articles": "/search/article",
            "thesis": "/search/thesis"
        }
        
        # ìš”ì²­ ê°„ê²© (ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘)
        self.request_delay = 0.5
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br'
        })
    
    async def execute_library_search(
        self, 
        query: str, 
        search_type: str = "integrated",
        search_field: str = "TOTAL",
        max_results: int = 20,
        year_from: Optional[str] = None,
        year_to: Optional[str] = None,
        additional_queries: Optional[List[Dict[str, str]]] = None,
        material_types: Optional[List[str]] = None,
        results_per_page: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ë„ì„œê´€ í†µí•©ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ì£¼ ê²€ìƒ‰ì–´
            search_type: ê²€ìƒ‰ ìœ í˜• (integrated, books, articles, thesis)
            search_field: ì£¼ ê²€ìƒ‰ì–´ì˜ ê²€ìƒ‰ í•­ëª©
                - "TOTAL": ì „ì²´
                - "1": ì„œëª…(ì±…ì œëª©)
                - "2": ì €ì
                - "3": ì¶œíŒì‚¬
                - "4": ì£¼ì œì–´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            year_from: ì‹œì‘ ì—°ë„ (ì˜ˆ: "2020")
            year_to: ì¢…ë£Œ ì—°ë„ (ì˜ˆ: "2025")
            additional_queries: ì¶”ê°€ ê²€ìƒ‰ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
                ê° í•­ëª©ì€ {"si": ê²€ìƒ‰í•„ë“œ, "q": ê²€ìƒ‰ì–´, "operator": ì—°ì‚°ì} í˜•ì‹
            material_types: ìë£Œìœ í˜• ë¦¬ìŠ¤íŠ¸ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)
                - "m": ë‹¨í–‰ë³¸
                - "s": ì—°ì†ê°„í–‰ë¬¼
                - "b;p;v;x;u;c": ë©€í‹°ë¯¸ë””ì–´/ë¹„ë„ì„œ
                - "t": í•™ìœ„ë…¼ë¬¸
                - "o": ê³ ì„œ
                - "zart": ê¸°ì‚¬
            results_per_page: ìª½ë‹¹ ì¶œë ¥ ê±´ìˆ˜ (5, 10, 15, 20, 30, 50, 100)
        
        Examples:
            # ê°„ë‹¨í•œ ê²€ìƒ‰: "íœ´ëŒ€í° OR ìŠ¤ë§ˆíŠ¸í° NOT ì•„ì´í°"
            await execute_library_search(
                query="íœ´ëŒ€í°",
                additional_queries=[
                    {"si": "TOTAL", "q": "ìŠ¤ë§ˆíŠ¸í°", "operator": "or"},
                    {"si": "TOTAL", "q": "ì•„ì´í°", "operator": "not"}
                ],
                year_from="2020",
                year_to="2025",
                results_per_page=100
            )
            
            # í•„ë“œë³„ ê²€ìƒ‰: ì„œëª…='íœ´ëŒ€í°' AND ì €ì='ê¹€ì² ìˆ˜' AND ì£¼ì œì–´='ì•„ì´í°'
            await execute_library_search(
                query="íœ´ëŒ€í°",
                search_field="1",  # ì„œëª…
                additional_queries=[
                    {"si": "2", "q": "ê¹€ì² ìˆ˜", "operator": "and"},  # ì €ì
                    {"si": "4", "q": "ì•„ì´í°", "operator": "and"}   # ì£¼ì œì–´
                ],
                year_from="2020",
                year_to="2025",
                results_per_page=100
            )
            
            # ìë£Œìœ í˜• ì„ íƒ: ì—°ì†ê°„í–‰ë¬¼ê³¼ í•™ìœ„ë…¼ë¬¸ë§Œ
            await execute_library_search(
                query="íœ´ëŒ€í°",
                material_types=["s", "t"]  # ì—°ì†ê°„í–‰ë¬¼, í•™ìœ„ë…¼ë¬¸
            )
        """
        
        try:
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = self._build_search_url(
                query, 
                search_type,
                search_field=search_field,
                year_from=year_from,
                year_to=year_to,
                additional_queries=additional_queries,
                material_types=material_types,
                results_per_page=results_per_page
            )
            
            logger.info(f"Executing library search: {search_url}")
            
            # ê²€ìƒ‰ ìš”ì²­
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            # ìœ¤ë¦¬ì  ì§€ì—°
            await asyncio.sleep(self.request_delay)
            
            # ê²°ê³¼ íŒŒì‹±
            search_results = self._parse_search_results(response.text, search_type)
            
            # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
            limited_results = search_results[:max_results]
            
            # ê° ê²°ê³¼ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            detailed_results = []
            for result in limited_results:
                try:
                    detailed_info = await self._get_detailed_info(result)
                    detailed_results.append(detailed_info)
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    await asyncio.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"Failed to get detailed info for {result.get('title', 'Unknown')}: {e}")
                    detailed_results.append(result)
            
            return detailed_results
            
        except Exception as e:
            logger.error(f"Library search failed: {e}")
            raise
    
    async def check_holdings(self, title: str, authors: Optional[List[str]] = None) -> Dict[str, Any]:
        """íŠ¹ì • ìë£Œì˜ ì†Œì¥ ì—¬ë¶€ ë° ì´ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        
        try:
            # ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
            query = f'title:"{title}"'
            if authors:
                author_query = " OR ".join([f'author:"{author}"' for author in authors[:2]])
                query += f" AND ({author_query})"
            
            search_results = await self.execute_library_search(query, max_results=5)
            
            if not search_results:
                return {
                    "status": "not_found",
                    "message": "í•´ë‹¹ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼ ì„ íƒ
            best_match = search_results[0]
            
            # ì†Œì¥ ì •ë³´ ìƒì„¸ ì¡°íšŒ
            holdings_info = await self._get_holdings_detail(best_match)
            
            return {
                "status": "found",
                "title": best_match.get("title"),
                "holdings": holdings_info,
                "access_info": self._generate_access_info(holdings_info)
            }
            
        except Exception as e:
            logger.error(f"Holdings check failed for '{title}': {e}")
            return {
                "status": "error",
                "message": f"ì†Œì¥ ì—¬ë¶€ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    async def get_available_databases(self) -> List[Dict[str, str]]:
        """ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ
            db_list_url = f"{self.base_url}/database"
            
            response = self.session.get(db_list_url, timeout=30)
            response.raise_for_status()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ íŒŒì‹±
            databases = self._parse_database_list(response.text)
            
            return databases
            
        except Exception as e:
            logger.error(f"Failed to get database list: {e}")
            # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜
            return self._get_default_databases()
    
    async def test_connection(self) -> Dict[str, Any]:
        """ë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        
        try:
            response = self.session.get(self.base_url, timeout=10)
            response.raise_for_status()
            
            return {
                "status": "success",
                "response_time": response.elapsed.total_seconds(),
                "status_code": response.status_code
            }
            
        except requests.exceptions.Timeout:
            return {
                "status": "timeout",
                "message": "ì—°ê²° ì‹œê°„ ì´ˆê³¼"
            }
        except requests.exceptions.ConnectionError:
            return {
                "status": "connection_error", 
                "message": "ì—°ê²° ì‹¤íŒ¨"
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e)
            }
    
    def _build_search_url(
        self, 
        query: str, 
        search_type: str,
        search_field: str = "TOTAL",
        year_from: Optional[str] = None,
        year_to: Optional[str] = None,
        additional_queries: Optional[List[Dict[str, str]]] = None,
        material_types: Optional[List[str]] = None,
        results_per_page: int = 10
    ) -> str:
        """
        ê²€ìƒ‰ URL êµ¬ì„±
        
        Args:
            query: ì£¼ ê²€ìƒ‰ì–´
            search_type: ê²€ìƒ‰ ìœ í˜• (integrated, books, articles, thesis)
            search_field: ì£¼ ê²€ìƒ‰ì–´ì˜ ê²€ìƒ‰ í•­ëª©
                - "TOTAL": ì „ì²´
                - "1": ì„œëª…(ì±…ì œëª©)
                - "2": ì €ì
                - "3": ì¶œíŒì‚¬
                - "4": ì£¼ì œì–´
            year_from: ì‹œì‘ ì—°ë„ (ì˜ˆ: "2020")
            year_to: ì¢…ë£Œ ì—°ë„ (ì˜ˆ: "2025")
            additional_queries: ì¶”ê°€ ê²€ìƒ‰ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: [{"si": "2", "q": "ê¹€ì² ìˆ˜", "operator": "and"},
                     {"si": "4", "q": "ì•„ì´í°", "operator": "and"}]
            material_types: ìë£Œìœ í˜• ë¦¬ìŠ¤íŠ¸ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)
                - "m": ë‹¨í–‰ë³¸
                - "s": ì—°ì†ê°„í–‰ë¬¼
                - "b;p;v;x;u;c": ë©€í‹°ë¯¸ë””ì–´/ë¹„ë„ì„œ
                - "t": í•™ìœ„ë…¼ë¬¸
                - "o": ê³ ì„œ
                - "zart": ê¸°ì‚¬
                - Noneì´ë©´ search_typeì— ë”°ë¼ ìë™ ê²°ì •
            results_per_page: ìª½ë‹¹ ì¶œë ¥ ê±´ìˆ˜ (5, 10, 15, 20, 30, 50, 100)
        
        Examples:
            # ì„œëª…='íœ´ëŒ€í°' AND ì €ì='ê¹€ì² ìˆ˜' AND ì£¼ì œì–´='ì•„ì´í°'
            _build_search_url(
                query="íœ´ëŒ€í°",
                search_field="1",  # ì„œëª…
                additional_queries=[
                    {"si": "2", "q": "ê¹€ì² ìˆ˜", "operator": "and"},  # ì €ì
                    {"si": "4", "q": "ì•„ì´í°", "operator": "and"}   # ì£¼ì œì–´
                ]
            )
            
            # ì—°ì†ê°„í–‰ë¬¼ê³¼ í•™ìœ„ë…¼ë¬¸ë§Œ ê²€ìƒ‰
            _build_search_url(
                query="íœ´ëŒ€í°",
                material_types=["s", "t"]  # ì—°ì†ê°„í–‰ë¬¼, í•™ìœ„ë…¼ë¬¸
            )
        """
        
        # í†µí•©ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì—”ë“œí¬ì¸íŠ¸
        endpoint = "/search/tot/result"
        
        # ê¸°ë³¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° êµ¬ì„± (ìˆœì„œ ì¤‘ìš”)
        params = []
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
        params.append(('st', 'KWRD'))
        params.append(('commandType', 'advanced'))
        
        # ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´ (ì£¼ ê²€ìƒ‰ì–´)
        params.append(('si', search_field))
        params.append(('q', query))
        
        # ì¶”ê°€ ê²€ìƒ‰ì–´ê°€ ìˆëŠ” ê²½ìš° (AND/OR/NOT ì—°ì‚°)
        if additional_queries:
            for idx, add_query in enumerate(additional_queries):
                operator = add_query.get('operator', 'and')  # and, or, not
                params.append((f'b{idx}', operator))
                params.append((f'weight{idx}', ''))
                params.append(('si', add_query.get('si', 'TOTAL')))
                params.append(('q', add_query.get('q', '')))
            
            # ë§ˆì§€ë§‰ weight íŒŒë¼ë¯¸í„°
            last_weight_idx = len(additional_queries)
            params.append((f'weight{last_weight_idx}', ''))
        
        # ìë£Œìœ í˜• ì œí•œ (lmt0)
        # material_typesê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ search_typeì— ë”°ë¼ ê²°ì •
        if material_types is None:
            if search_type == "books":
                selected_material_types = ['m']  # ë‹¨í–‰ë³¸
            elif search_type == "articles":
                selected_material_types = ['zart']  # ê¸°ì‚¬
            elif search_type == "thesis":
                selected_material_types = ['t']  # í•™ìœ„ë…¼ë¬¸
            else:
                selected_material_types = ['TOTAL']  # ì „ì²´
        else:
            selected_material_types = material_types
        
        # ìë£Œìœ í˜• íŒŒë¼ë¯¸í„° êµ¬ì„± (HTML form ìˆœì„œ ë°˜ì˜)
        # ìˆœì„œ: TOTAL, m(ë‹¨í–‰ë³¸), s(ì—°ì†ê°„í–‰ë¬¼), b;p;v;x;u;c(ë©€í‹°ë¯¸ë””ì–´), t(í•™ìœ„ë…¼ë¬¸), o(ê³ ì„œ), zart(ê¸°ì‚¬)
        material_type_order = ['TOTAL', 'm', 's', 'b;p;v;x;u;c', 't', 'o', 'zart']
        
        # ì²« ë²ˆì§¸ _lmt0 (í•­ìƒ on)
        params.append(('_lmt0', 'on'))
        params.append(('lmtsn', '000000000001'))
        params.append(('lmtst', 'OR'))
        
        # ì„ íƒëœ ìë£Œìœ í˜•ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¶”ê°€
        for mat_type in material_type_order:
            params.append(('_lmt0', 'on'))
            if mat_type in selected_material_types:
                params.append(('lmt0', mat_type))
        
        # ìˆ˜ë¡ë§¤ì²´ ì œí•œ (inc)
        params.append(('inc', 'TOTAL'))
        for _ in range(6):
            params.append(('_inc', 'on'))
        
        # ì–¸ì–´ ì œí•œ (lmt1)
        params.append(('lmt1', 'TOTAL'))
        params.append(('lmtsn', '000000000003'))
        params.append(('lmtst', 'OR'))
        
        # ì†Œì¥ì²˜ ì œí•œ (lmt2) - ì‹ ì´Œ+êµ­ì œ
        params.append(('lmt2', 'YNLIB;GSISL;MUSEL;OTHER;UGSTL;YSLIB;ARCHL;BUSIL;KORCL;IOKSL;LAWSL;MULTL;MATHL;MUSIC;UML'))
        params.append(('lmtsn', '000000000006'))
        params.append(('lmtst', 'OR'))
        
        # ë°œí–‰ë…„ë„ ë²”ìœ„ ì„¤ì •
        if year_from:
            params.append(('rf', year_from))
        if year_to:
            params.append(('rt', year_to))
        if year_from or year_to:
            params.append(('range', '000000000021'))
        
        # í˜ì´ì§• ì„¤ì •
        params.append(('cpp', str(results_per_page)))  # ìª½ë‹¹ ì¶œë ¥ ê±´ìˆ˜
        params.append(('msc', '10000'))  # ìµœëŒ€ ê²€ìƒ‰ ê±´ìˆ˜
        
        # URL íŒŒë¼ë¯¸í„° ë¬¸ìì—´ êµ¬ì„±
        param_string = "&".join([f"{k}={quote(str(v))}" for k, v in params])
        
        return f"{self.base_url}{endpoint}?{param_string}"
    
    def _parse_search_results(self, html_content: str, search_type: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ í•­ëª© ì„ íƒì (ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        result_items = soup.select('.search-result-item, .list-item, .result-item')
        
        for item in result_items:
            try:
                result = self._extract_result_info(item, search_type)
                if result:
                    results.append(result)
            except Exception as e:
                logger.warning(f"Failed to parse result item: {e}")
                continue
        
        return results
    
    def _extract_result_info(self, item_element, search_type: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ"""
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = item_element.select_one('.title, .item-title, h3, h4')
            title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
            
            # ì €ì ì¶”ì¶œ
            author_elem = item_element.select_one('.author, .item-author, .creator')
            authors = []
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                authors = [author.strip() for author in author_text.split(',')]
            
            # ì¶œíŒ ì •ë³´ ì¶”ì¶œ
            pub_elem = item_element.select_one('.publication, .pub-info, .publisher')
            publication_info = pub_elem.get_text(strip=True) if pub_elem else ""
            
            # ì—°ë„ ì¶”ì¶œ
            year = self._extract_year(publication_info + " " + title)
            
            # ìƒì„¸ ë§í¬ ì¶”ì¶œ
            link_elem = item_element.select_one('a[href]')
            detail_link = ""
            if link_elem:
                href = link_elem.get('href')
                detail_link = urljoin(self.base_url, href) if href else ""
            
            # ìë£Œ ìœ í˜• ì¶”ì¶œ
            type_elem = item_element.select_one('.type, .material-type, .format')
            material_type = type_elem.get_text(strip=True) if type_elem else "ê¸°íƒ€"
            
            return {
                "title": title,
                "authors": authors,
                "publication_info": publication_info,
                "year": year,
                "material_type": material_type,
                "detail_link": detail_link,
                "search_type": search_type
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract result info: {e}")
            return None
    
    async def _get_detailed_info(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        
        if not result.get('detail_link'):
            return result
        
        try:
            response = self.session.get(result['detail_link'], timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì´ˆë¡ ì¶”ì¶œ
            abstract_elem = soup.select_one('.abstract, .summary, .description')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_elem = soup.select_one('.keywords, .subjects, .tags')
            keywords = []
            if keywords_elem:
                keyword_text = keywords_elem.get_text(strip=True)
                keywords = [kw.strip() for kw in keyword_text.split(',')]
            
            # ì†Œì¥ ì •ë³´ ì¶”ì¶œ
            holdings = self._extract_holdings_info(soup)
            
            # ì›ë¬¸ ë§í¬ ì¶”ì¶œ
            fulltext_elem = soup.select_one('.fulltext-link, .pdf-link, .online-access')
            fulltext_link = ""
            if fulltext_elem:
                href = fulltext_elem.get('href')
                fulltext_link = urljoin(self.base_url, href) if href else ""
            
            # ê¸°ì¡´ ê²°ê³¼ì— ìƒì„¸ ì •ë³´ ì¶”ê°€
            result.update({
                "abstract": abstract[:500] + "..." if len(abstract) > 500 else abstract,
                "keywords": keywords,
                "holdings": holdings,
                "fulltext_link": fulltext_link
            })
            
            return result
            
        except Exception as e:
            logger.warning(f"Failed to get detailed info: {e}")
            return result
    
    async def _get_holdings_detail(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
        
        holdings = result.get('holdings', {})
        
        # ê¸°ë³¸ ì†Œì¥ ì •ë³´ê°€ ì—†ìœ¼ë©´ Mock ë°ì´í„° ìƒì„±
        if not holdings:
            return self._generate_mock_holdings()
        
        return holdings
    
    def _extract_holdings_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ì¶”ì¶œ"""
        
        holdings = {
            "locations": [],
            "status": "available",
            "loan_status": "ëŒ€ì¶œ ê°€ëŠ¥",
            "access_type": "physical"
        }
        
        # ì†Œì¥ ìœ„ì¹˜ ì¶”ì¶œ
        location_elems = soup.select('.location, .library-location, .holdings-location')
        for loc_elem in location_elems:
            location_text = loc_elem.get_text(strip=True)
            if location_text:
                holdings["locations"].append(location_text)
        
        # ëŒ€ì¶œ ìƒíƒœ ì¶”ì¶œ
        status_elem = soup.select_one('.status, .availability, .loan-status')
        if status_elem:
            status_text = status_elem.get_text(strip=True)
            holdings["loan_status"] = status_text
            
            # ìƒíƒœì— ë”°ë¥¸ ê°€ìš©ì„± íŒë‹¨
            if any(keyword in status_text for keyword in ["ëŒ€ì¶œì¤‘", "ì´ìš©ë¶ˆê°€", "ë¶„ì‹¤"]):
                holdings["status"] = "unavailable"
        
        # ì˜¨ë¼ì¸ ì ‘ê·¼ ì—¬ë¶€ í™•ì¸
        online_elem = soup.select_one('.online-access, .electronic-resource, .e-resource')
        if online_elem:
            holdings["access_type"] = "electronic"
            holdings["loan_status"] = "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        
        return holdings
    
    def _generate_access_info(self, holdings: Dict[str, Any]) -> str:
        """ì ‘ê·¼ ì •ë³´ ìƒì„±"""
        
        access_type = holdings.get("access_type", "physical")
        locations = holdings.get("locations", [])
        loan_status = holdings.get("loan_status", "")
        
        if access_type == "electronic":
            return "âœ… ì „ì ì €ë„ ì›ë¬¸ ì´ìš© ê°€ëŠ¥"
        elif locations:
            location_str = ", ".join(locations[:2])  # ìµœëŒ€ 2ê°œ ìœ„ì¹˜ë§Œ í‘œì‹œ
            return f"ğŸ“š {location_str} - {loan_status}"
        else:
            return f"ğŸ“– {loan_status}"
    
    def _generate_mock_holdings(self) -> Dict[str, Any]:
        """Mock ì†Œì¥ ì •ë³´ ìƒì„±"""
        import random
        
        mock_locations = [
            "ì¤‘ì•™ë„ì„œê´€ 3ì¸µ",
            "í•™ìˆ ì •ë³´ì› 2ì¸µ", 
            "ê³¼í•™ë„ì„œê´€ 1ì¸µ",
            "ì˜í•™ë„ì„œê´€"
        ]
        
        mock_statuses = [
            "ëŒ€ì¶œ ê°€ëŠ¥",
            "ëŒ€ì¶œì¤‘",
            "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        ]
        
        return {
            "locations": [random.choice(mock_locations)],
            "status": "available",
            "loan_status": random.choice(mock_statuses),
            "access_type": random.choice(["physical", "electronic"])
        }
    
    def _parse_database_list(self, html_content: str) -> List[Dict[str, str]]:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ íŒŒì‹±"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        databases = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ í•­ëª© ì„ íƒ
        db_items = soup.select('.database-item, .db-list-item, .resource-item')
        
        for item in db_items:
            try:
                name_elem = item.select_one('.db-name, .resource-name, h3, h4')
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    
                    # ì„¤ëª… ì¶”ì¶œ
                    desc_elem = item.select_one('.description, .db-desc')
                    description = desc_elem.get_text(strip=True) if desc_elem else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.select_one('a[href]')
                    url = link_elem.get('href') if link_elem else ""
                    
                    databases.append({
                        "name": name,
                        "description": description[:100] + "..." if len(description) > 100 else description,
                        "url": urljoin(self.base_url, url) if url else ""
                    })
                    
            except Exception as e:
                logger.warning(f"Failed to parse database item: {e}")
                continue
        
        return databases
    
    def _get_default_databases(self) -> List[Dict[str, str]]:
        """ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡"""
        return [
            {
                "name": "KISS (í•œêµ­í•™ìˆ ì •ë³´)",
                "description": "í•œêµ­ í•™ìˆ ë…¼ë¬¸ ë° í•™ìœ„ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://kiss.kstudy.com"
            },
            {
                "name": "DBpia",
                "description": "êµ­ë‚´ í•™ìˆ ë…¼ë¬¸ ë° ì „ë¬¸ìë£Œ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://www.dbpia.co.kr"
            },
            {
                "name": "RISS",
                "description": "êµ­ë‚´ í•™ìˆ ì—°êµ¬ì •ë³´ì„œë¹„ìŠ¤",
                "url": "http://www.riss.kr"
            },
            {
                "name": "Web of Science",
                "description": "êµ­ì œ í•™ìˆ ë…¼ë¬¸ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://webofscience.com"
            }
        ]
    
    def _extract_year(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        import re
        
        # 4ìë¦¬ ì—°ë„ íŒ¨í„´ ì°¾ê¸°
        year_pattern = r'\b(19|20)\d{2}\b'
        matches = re.findall(year_pattern, text)
        
        if matches:
            # ê°€ì¥ ìµœê·¼ ì—°ë„ ë°˜í™˜
            years = [int(match + m[2:]) for match, m in re.findall(r'\b(19|20)(\d{2})\b', text)]
            return max(years) if years else 0
        
        return 0
    
    def __del__(self):
        """ì†Œë©¸ì: ì„¸ì…˜ ì •ë¦¬"""
        if hasattr(self, 'session'):
            self.session.close()