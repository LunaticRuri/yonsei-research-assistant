import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, quote
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

class LibraryScraper:
    """ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    
    def __init__(self):
        self.base_url = "https://library.yonsei.ac.kr"
        self.search_endpoints = {
            "integrated": "/search/tot",
            "books": "/search/book", 
            "articles": "/search/article",
            "thesis": "/search/thesis"
        }
        
        # ìš”ì²­ ê°„ê²© (ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘)
        self.request_delay = 1.0
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br'
        })
    
    async def execute_library_search(
        self, 
        query: str, 
        search_type: str = "integrated",
        max_results: int = 20
    ) -> List[Dict[str, Any]]:
        """ë„ì„œê´€ í†µí•©ê²€ìƒ‰ ì‹¤í–‰"""
        
        try:
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = self._build_search_url(query, search_type)
            
            logger.info(f"Executing library search: {search_url}")
            
            # ê²€ìƒ‰ ìš”ì²­
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            # ìœ¤ë¦¬ì  ì§€ì—°
            await asyncio.sleep(self.request_delay)
            
            # ê²°ê³¼ íŒŒì‹±
            search_results = self._parse_search_results(response.text, search_type)
            
            # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
            limited_results = search_results[:max_results]
            
            # ê° ê²°ê³¼ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            detailed_results = []
            for result in limited_results:
                try:
                    detailed_info = await self._get_detailed_info(result)
                    detailed_results.append(detailed_info)
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    await asyncio.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"Failed to get detailed info for {result.get('title', 'Unknown')}: {e}")
                    detailed_results.append(result)
            
            return detailed_results
            
        except Exception as e:
            logger.error(f"Library search failed: {e}")
            raise
    
    async def check_holdings(self, title: str, authors: Optional[List[str]] = None) -> Dict[str, Any]:
        """íŠ¹ì • ìë£Œì˜ ì†Œì¥ ì—¬ë¶€ ë° ì´ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        
        try:
            # ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
            query = f'title:"{title}"'
            if authors:
                author_query = " OR ".join([f'author:"{author}"' for author in authors[:2]])
                query += f" AND ({author_query})"
            
            search_results = await self.execute_library_search(query, max_results=5)
            
            if not search_results:
                return {
                    "status": "not_found",
                    "message": "í•´ë‹¹ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼ ì„ íƒ
            best_match = search_results[0]
            
            # ì†Œì¥ ì •ë³´ ìƒì„¸ ì¡°íšŒ
            holdings_info = await self._get_holdings_detail(best_match)
            
            return {
                "status": "found",
                "title": best_match.get("title"),
                "holdings": holdings_info,
                "access_info": self._generate_access_info(holdings_info)
            }
            
        except Exception as e:
            logger.error(f"Holdings check failed for '{title}': {e}")
            return {
                "status": "error",
                "message": f"ì†Œì¥ ì—¬ë¶€ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    async def get_available_databases(self) -> List[Dict[str, str]]:
        """ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ
            db_list_url = f"{self.base_url}/database"
            
            response = self.session.get(db_list_url, timeout=30)
            response.raise_for_status()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ íŒŒì‹±
            databases = self._parse_database_list(response.text)
            
            return databases
            
        except Exception as e:
            logger.error(f"Failed to get database list: {e}")
            # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜
            return self._get_default_databases()
    
    async def test_connection(self) -> Dict[str, Any]:
        """ë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        
        try:
            response = self.session.get(self.base_url, timeout=10)
            response.raise_for_status()
            
            return {
                "status": "success",
                "response_time": response.elapsed.total_seconds(),
                "status_code": response.status_code
            }
            
        except requests.exceptions.Timeout:
            return {
                "status": "timeout",
                "message": "ì—°ê²° ì‹œê°„ ì´ˆê³¼"
            }
        except requests.exceptions.ConnectionError:
            return {
                "status": "connection_error", 
                "message": "ì—°ê²° ì‹¤íŒ¨"
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e)
            }
    
    def _build_search_url(self, query: str, search_type: str) -> str:
        """ê²€ìƒ‰ URL êµ¬ì„±"""
        
        endpoint = self.search_endpoints.get(search_type, self.search_endpoints["integrated"])
        encoded_query = quote(query)
        
        # ê¸°ë³¸ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
        params = {
            'q': encoded_query,
            'page': 1,
            'size': 20,
            'sort': 'relevance'
        }
        
        # URL ë§¤ê°œë³€ìˆ˜ ë¬¸ìì—´ êµ¬ì„±
        param_string = "&".join([f"{k}={v}" for k, v in params.items()])
        
        return f"{self.base_url}{endpoint}?{param_string}"
    
    def _parse_search_results(self, html_content: str, search_type: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ í•­ëª© ì„ íƒì (ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        result_items = soup.select('.search-result-item, .list-item, .result-item')
        
        for item in result_items:
            try:
                result = self._extract_result_info(item, search_type)
                if result:
                    results.append(result)
            except Exception as e:
                logger.warning(f"Failed to parse result item: {e}")
                continue
        
        return results
    
    def _extract_result_info(self, item_element, search_type: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ"""
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = item_element.select_one('.title, .item-title, h3, h4')
            title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
            
            # ì €ì ì¶”ì¶œ
            author_elem = item_element.select_one('.author, .item-author, .creator')
            authors = []
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                authors = [author.strip() for author in author_text.split(',')]
            
            # ì¶œíŒ ì •ë³´ ì¶”ì¶œ
            pub_elem = item_element.select_one('.publication, .pub-info, .publisher')
            publication_info = pub_elem.get_text(strip=True) if pub_elem else ""
            
            # ì—°ë„ ì¶”ì¶œ
            year = self._extract_year(publication_info + " " + title)
            
            # ìƒì„¸ ë§í¬ ì¶”ì¶œ
            link_elem = item_element.select_one('a[href]')
            detail_link = ""
            if link_elem:
                href = link_elem.get('href')
                detail_link = urljoin(self.base_url, href) if href else ""
            
            # ìë£Œ ìœ í˜• ì¶”ì¶œ
            type_elem = item_element.select_one('.type, .material-type, .format')
            material_type = type_elem.get_text(strip=True) if type_elem else "ê¸°íƒ€"
            
            return {
                "title": title,
                "authors": authors,
                "publication_info": publication_info,
                "year": year,
                "material_type": material_type,
                "detail_link": detail_link,
                "search_type": search_type
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract result info: {e}")
            return None
    
    async def _get_detailed_info(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        
        if not result.get('detail_link'):
            return result
        
        try:
            response = self.session.get(result['detail_link'], timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì´ˆë¡ ì¶”ì¶œ
            abstract_elem = soup.select_one('.abstract, .summary, .description')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_elem = soup.select_one('.keywords, .subjects, .tags')
            keywords = []
            if keywords_elem:
                keyword_text = keywords_elem.get_text(strip=True)
                keywords = [kw.strip() for kw in keyword_text.split(',')]
            
            # ì†Œì¥ ì •ë³´ ì¶”ì¶œ
            holdings = self._extract_holdings_info(soup)
            
            # ì›ë¬¸ ë§í¬ ì¶”ì¶œ
            fulltext_elem = soup.select_one('.fulltext-link, .pdf-link, .online-access')
            fulltext_link = ""
            if fulltext_elem:
                href = fulltext_elem.get('href')
                fulltext_link = urljoin(self.base_url, href) if href else ""
            
            # ê¸°ì¡´ ê²°ê³¼ì— ìƒì„¸ ì •ë³´ ì¶”ê°€
            result.update({
                "abstract": abstract[:500] + "..." if len(abstract) > 500 else abstract,
                "keywords": keywords,
                "holdings": holdings,
                "fulltext_link": fulltext_link
            })
            
            return result
            
        except Exception as e:
            logger.warning(f"Failed to get detailed info: {e}")
            return result
    
    async def _get_holdings_detail(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
        
        holdings = result.get('holdings', {})
        
        # ê¸°ë³¸ ì†Œì¥ ì •ë³´ê°€ ì—†ìœ¼ë©´ Mock ë°ì´í„° ìƒì„±
        if not holdings:
            return self._generate_mock_holdings()
        
        return holdings
    
    def _extract_holdings_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ì¶”ì¶œ"""
        
        holdings = {
            "locations": [],
            "status": "available",
            "loan_status": "ëŒ€ì¶œ ê°€ëŠ¥",
            "access_type": "physical"
        }
        
        # ì†Œì¥ ìœ„ì¹˜ ì¶”ì¶œ
        location_elems = soup.select('.location, .library-location, .holdings-location')
        for loc_elem in location_elems:
            location_text = loc_elem.get_text(strip=True)
            if location_text:
                holdings["locations"].append(location_text)
        
        # ëŒ€ì¶œ ìƒíƒœ ì¶”ì¶œ
        status_elem = soup.select_one('.status, .availability, .loan-status')
        if status_elem:
            status_text = status_elem.get_text(strip=True)
            holdings["loan_status"] = status_text
            
            # ìƒíƒœì— ë”°ë¥¸ ê°€ìš©ì„± íŒë‹¨
            if any(keyword in status_text for keyword in ["ëŒ€ì¶œì¤‘", "ì´ìš©ë¶ˆê°€", "ë¶„ì‹¤"]):
                holdings["status"] = "unavailable"
        
        # ì˜¨ë¼ì¸ ì ‘ê·¼ ì—¬ë¶€ í™•ì¸
        online_elem = soup.select_one('.online-access, .electronic-resource, .e-resource')
        if online_elem:
            holdings["access_type"] = "electronic"
            holdings["loan_status"] = "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        
        return holdings
    
    def _generate_access_info(self, holdings: Dict[str, Any]) -> str:
        """ì ‘ê·¼ ì •ë³´ ìƒì„±"""
        
        access_type = holdings.get("access_type", "physical")
        locations = holdings.get("locations", [])
        loan_status = holdings.get("loan_status", "")
        
        if access_type == "electronic":
            return "âœ… ì „ì ì €ë„ ì›ë¬¸ ì´ìš© ê°€ëŠ¥"
        elif locations:
            location_str = ", ".join(locations[:2])  # ìµœëŒ€ 2ê°œ ìœ„ì¹˜ë§Œ í‘œì‹œ
            return f"ğŸ“š {location_str} - {loan_status}"
        else:
            return f"ğŸ“– {loan_status}"
    
    def _generate_mock_holdings(self) -> Dict[str, Any]:
        """Mock ì†Œì¥ ì •ë³´ ìƒì„±"""
        import random
        
        mock_locations = [
            "ì¤‘ì•™ë„ì„œê´€ 3ì¸µ",
            "í•™ìˆ ì •ë³´ì› 2ì¸µ", 
            "ê³¼í•™ë„ì„œê´€ 1ì¸µ",
            "ì˜í•™ë„ì„œê´€"
        ]
        
        mock_statuses = [
            "ëŒ€ì¶œ ê°€ëŠ¥",
            "ëŒ€ì¶œì¤‘",
            "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        ]
        
        return {
            "locations": [random.choice(mock_locations)],
            "status": "available",
            "loan_status": random.choice(mock_statuses),
            "access_type": random.choice(["physical", "electronic"])
        }
    
    def _parse_database_list(self, html_content: str) -> List[Dict[str, str]]:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ íŒŒì‹±"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        databases = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ í•­ëª© ì„ íƒ
        db_items = soup.select('.database-item, .db-list-item, .resource-item')
        
        for item in db_items:
            try:
                name_elem = item.select_one('.db-name, .resource-name, h3, h4')
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    
                    # ì„¤ëª… ì¶”ì¶œ
                    desc_elem = item.select_one('.description, .db-desc')
                    description = desc_elem.get_text(strip=True) if desc_elem else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.select_one('a[href]')
                    url = link_elem.get('href') if link_elem else ""
                    
                    databases.append({
                        "name": name,
                        "description": description[:100] + "..." if len(description) > 100 else description,
                        "url": urljoin(self.base_url, url) if url else ""
                    })
                    
            except Exception as e:
                logger.warning(f"Failed to parse database item: {e}")
                continue
        
        return databases
    
    def _get_default_databases(self) -> List[Dict[str, str]]:
        """ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡"""
        return [
            {
                "name": "KISS (í•œêµ­í•™ìˆ ì •ë³´)",
                "description": "í•œêµ­ í•™ìˆ ë…¼ë¬¸ ë° í•™ìœ„ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://kiss.kstudy.com"
            },
            {
                "name": "DBpia",
                "description": "êµ­ë‚´ í•™ìˆ ë…¼ë¬¸ ë° ì „ë¬¸ìë£Œ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://www.dbpia.co.kr"
            },
            {
                "name": "RISS",
                "description": "êµ­ë‚´ í•™ìˆ ì—°êµ¬ì •ë³´ì„œë¹„ìŠ¤",
                "url": "http://www.riss.kr"
            },
            {
                "name": "Web of Science",
                "description": "êµ­ì œ í•™ìˆ ë…¼ë¬¸ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤",
                "url": "https://webofscience.com"
            }
        ]
    
    def _extract_year(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        import re
        
        # 4ìë¦¬ ì—°ë„ íŒ¨í„´ ì°¾ê¸°
        year_pattern = r'\b(19|20)\d{2}\b'
        matches = re.findall(year_pattern, text)
        
        if matches:
            # ê°€ì¥ ìµœê·¼ ì—°ë„ ë°˜í™˜
            years = [int(match + m[2:]) for match, m in re.findall(r'\b(19|20)(\d{2})\b', text)]
            return max(years) if years else 0
        
        return 0
    
    def __del__(self):
        """ì†Œë©¸ì: ì„¸ì…˜ ì •ë¦¬"""
        if hasattr(self, 'session'):
            self.session.close()