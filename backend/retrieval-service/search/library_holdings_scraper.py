import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Any, Optional, Literal
from urllib.parse import urljoin, quote
import asyncio
import aiohttp
from pydantic import BaseModel, Field, field_validator
from enum import Enum

logger = logging.getLogger(__name__)


# ============================================================================
# Pydantic Models for Library Search Parameters
# ============================================================================

class SearchField(str, Enum):
    """ê²€ìƒ‰ í•„ë“œ íƒ€ì…"""
    TOTAL = "TOTAL"  # ì „ì²´
    TITLE = "1"  # ì„œëª…(ì±…ì œëª©)
    AUTHOR = "2"  # ì €ì
    PUBLISHER = "3"  # ì¶œíŒì‚¬
    SUBJECT = "4"  # ì£¼ì œì–´


class MaterialType(str, Enum):
    """ìë£Œ ìœ í˜•"""
    TOTAL = "TOTAL"  # ì „ì²´
    BOOK = "m"  # ë‹¨í–‰ë³¸
    SERIAL = "s"  # ì—°ì†ê°„í–‰ë¬¼
    MULTIMEDIA = "b;p;v;x;u;c"  # ë©€í‹°ë¯¸ë””ì–´/ë¹„ë„ì„œ
    THESIS = "t"  # í•™ìœ„ë…¼ë¬¸
    OLD_BOOK = "o"  # ê³ ì„œ
    ARTICLE = "zart"  # ê¸°ì‚¬


class QueryOperator(str, Enum):
    """ê²€ìƒ‰ ì—°ì‚°ì"""
    AND = "and"
    OR = "or"
    NOT = "not"


class AdditionalQuery(BaseModel):
    """ì¶”ê°€ ê²€ìƒ‰ ì¡°ê±´"""
    search_field: SearchField = Field(
        default=SearchField.TOTAL,
        description="ê²€ìƒ‰í•  í•„ë“œ (ì „ì²´, ì„œëª…, ì €ì, ì¶œíŒì‚¬, ì£¼ì œì–´)"
    )
    query: str = Field(
        ...,
        min_length=1,
        description="ê²€ìƒ‰ì–´"
    )
    operator: QueryOperator = Field(
        default=QueryOperator.AND,
        description="ì´ì „ ê²€ìƒ‰ì–´ì™€ì˜ ì—°ì‚°ì (AND, OR, NOT)"
    )
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "search_field": "AUTHOR",
                    "query": "ê¹€ì² ìˆ˜",
                    "operator": "AND"
                }
            ]
        }
    }


class YearRange(BaseModel):
    """ë°œí–‰ ì—°ë„ ë²”ìœ„"""
    from_year: Optional[int] = Field(
        default=None,
        ge=1900,
        le=2100,
        description="ì‹œì‘ ì—°ë„"
    )
    to_year: Optional[int] = Field(
        default=None,
        ge=1900,
        le=2100,
        description="ì¢…ë£Œ ì—°ë„"
    )
    
    @field_validator('to_year')
    @classmethod
    def validate_year_range(cls, v, info):
        """ì¢…ë£Œ ì—°ë„ê°€ ì‹œì‘ ì—°ë„ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ì§€ ê²€ì¦"""
        if v is not None and info.data.get('from_year') is not None:
            if v < info.data['from_year']:
                raise ValueError('ì¢…ë£Œ ì—°ë„ëŠ” ì‹œì‘ ì—°ë„ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤')
        return v
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {"from_year": 2020, "to_year": 2025}
            ]
        }
    }


class LibraryHoldingsSearchParams(BaseModel):
    """ë„ì„œê´€ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
    
    Examples:
        # ê°„ë‹¨í•œ ê²€ìƒ‰
        >>> params = LibrarySearchParams(
        ...     query="íœ´ëŒ€í°",
        ...     additional_queries=[
        ...         AdditionalQuery(query="ìŠ¤ë§ˆíŠ¸í°", operator=QueryOperator.OR),
        ...         AdditionalQuery(query="ì•„ì´í°", operator=QueryOperator.NOT)
        ...     ],
        ...     year_range=YearRange(from_year=2020, to_year=2025),
        ...     results_per_page=100
        ... )
        
        # í•„ë“œë³„ ê²€ìƒ‰
        >>> params = LibrarySearchParams(
        ...     query="íœ´ëŒ€í°",
        ...     search_field=SearchField.TITLE,
        ...     additional_queries=[
        ...         AdditionalQuery(
        ...             search_field=SearchField.AUTHOR,
        ...             query="ê¹€ì² ìˆ˜",
        ...             operator=QueryOperator.AND
        ...         )
        ...     ]
        ... )
        
        # ìë£Œ ìœ í˜• ì„ íƒ
        >>> params = LibrarySearchParams(
        ...     query="íœ´ëŒ€í°",
        ...     material_types=[MaterialType.SERIAL, MaterialType.THESIS]
        ... )
    """
    
    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    query: str = Field(
        ...,
        min_length=1,
        description="ì£¼ ê²€ìƒ‰ì–´"
    )
    
    # ê²€ìƒ‰ ì˜µì…˜
    search_field: SearchField = Field(
        default=SearchField.TOTAL,
        description="ì£¼ ê²€ìƒ‰ì–´ì˜ ê²€ìƒ‰ í•„ë“œ"
    )
    
    # ì¶”ê°€ ê²€ìƒ‰ ì¡°ê±´
    additional_queries: List[AdditionalQuery] = Field(
        default_factory=list,
        max_length=10,
        description="ì¶”ê°€ ê²€ìƒ‰ ì¡°ê±´ (ìµœëŒ€ 10ê°œ)"
    )
    
    # í•„í„°ë§ ì˜µì…˜
    material_types: List[MaterialType] = Field(
        default=[MaterialType.TOTAL],
        min_length=1,
        description="ê²€ìƒ‰í•  ìë£Œ ìœ í˜• (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)"
    )
    
    year_range: Optional[YearRange] = Field(
        default=None,
        description="ë°œí–‰ ì—°ë„ ë²”ìœ„"
    )
    
    # í˜ì´ì§• ì˜µì…˜
    results_per_page: Literal[5, 10, 15, 20, 30, 50, 100] = Field(
        default=10,
        description="í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜"
    )
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "query": "íœ´ëŒ€í°",
                    "search_field": "TITLE",
                    "additional_queries": [
                        {
                            "search_field": "AUTHOR",
                            "query": "ê¹€ì² ìˆ˜",
                            "operator": "AND"
                        },
                        {
                            "search_field": "SUBJECT",
                            "query": "ì•„ì´í°",
                            "operator": "AND"
                        }
                    ],
                    "material_types": ["SERIAL", "THESIS"],
                    "year_range": {
                        "from_year": 2020,
                        "to_year": 2025
                    },
                    "results_per_page": 100
                }
            ]
        }
    }


class LibraryHoldingsScraper:
    """ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    
    def __init__(self):
        self.base_url = "https://library.yonsei.ac.kr"
        
        # ìš”ì²­ ê°„ê²© (ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘)
        self.request_delay = 0.5
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br'
        })
    
    async def execute_holdings_search(
        self, 
        params: LibraryHoldingsSearchParams,
        search_type: str = "integrated",
        max_results: int = 20
    ) -> List[Dict[str, Any]]:
        """
        ë„ì„œê´€ í†µí•©ê²€ìƒ‰ ì‹¤í–‰ (Pydantic ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            params: LibrarySearchParams ê°ì²´ë¡œ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            search_type: ê²€ìƒ‰ ìœ í˜• (integrated, books, articles, thesis)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Examples:
            # ê°„ë‹¨í•œ ê²€ìƒ‰
            >>> params = LibrarySearchParams(
            ...     query="íœ´ëŒ€í°",
            ...     additional_queries=[
            ...         AdditionalQuery(query="ìŠ¤ë§ˆíŠ¸í°", operator=QueryOperator.OR),
            ...         AdditionalQuery(query="ì•„ì´í°", operator=QueryOperator.NOT)
            ...     ],
            ...     year_range=YearRange(from_year=2020, to_year=2025),
            ...     results_per_page=100
            ... )
            >>> results = await scraper.execute_holdings_search(params)
            
            # í•„ë“œë³„ ê²€ìƒ‰
            >>> params = LibrarySearchParams(
            ...     query="íœ´ëŒ€í°",
            ...     search_field=SearchField.TITLE,
            ...     additional_queries=[
            ...         AdditionalQuery(
            ...             search_field=SearchField.AUTHOR,
            ...             query="ê¹€ì² ìˆ˜",
            ...             operator=QueryOperator.AND
            ...         )
            ...     ]
            ... )
            >>> results = await scraper.execute_holdings_search(params)
            
            # ìë£Œ ìœ í˜• ì„ íƒ
            >>> params = LibrarySearchParams(
            ...     query="íœ´ëŒ€í°",
            ...     material_types=[MaterialType.SERIAL, MaterialType.THESIS]
            ... )
            >>> results = await scraper.execute_holdings_search(params)
        """
        
        try:
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = self._build_holdings_search_url(params)
            
            logger.info(f"Executing holdings search: {search_url}")
            
            # ê²€ìƒ‰ ìš”ì²­
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            # ìœ¤ë¦¬ì  ì§€ì—°
            await asyncio.sleep(self.request_delay)
            
            # ê²°ê³¼ íŒŒì‹±
            search_results = self._parse_holdings_search_results(response.text, search_type)
            
            # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
            limited_results = search_results[:max_results]
            
            # ê° ê²°ê³¼ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            detailed_results = []
            for result in limited_results:
                try:
                    detailed_info = await self._get_detailed_info(result)
                    detailed_results.append(detailed_info)
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    await asyncio.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"Failed to get detailed info for {result.get('title', 'Unknown')}: {e}")
                    detailed_results.append(result)
            
            return detailed_results
            
        except Exception as e:
            logger.error(f"Library search failed: {e}")
            raise
    
    def _build_holdings_search_url(self, params: LibraryHoldingsSearchParams) -> str:
        """
        ê²€ìƒ‰ URL êµ¬ì„± (Pydantic ê¸°ë°˜)
        
        Args:
            params: LibrarySearchParams ê°ì²´ë¡œ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        
        Returns:
            str: êµ¬ì„±ëœ ê²€ìƒ‰ URL
        
        Examples:
            >>> params = LibrarySearchParams(
            ...     query="íœ´ëŒ€í°",
            ...     search_field=SearchField.TITLE,
            ...     additional_queries=[
            ...         AdditionalQuery(search_field=SearchField.AUTHOR, query="ê¹€ì² ìˆ˜")
            ...     ],
            ...     material_types=[MaterialType.SERIAL, MaterialType.THESIS],
            ...     year_range=YearRange(from_year=2020, to_year=2025),
            ...     results_per_page=100
            ... )
            >>> url = scraper._build_holdings_search_url(params)
        """
        
        # í†µí•©ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì—”ë“œí¬ì¸íŠ¸
        endpoint = "/search/tot/result"
        
        # ê¸°ë³¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° êµ¬ì„± (ìˆœì„œ ì¤‘ìš”)
        url_params = []
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
        url_params.append(('st', 'KWRD'))
        url_params.append(('commandType', 'advanced'))
        
        # ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´ (ì£¼ ê²€ìƒ‰ì–´)
        url_params.append(('si', params.search_field.value))
        url_params.append(('q', params.query))
        
        # ì¶”ê°€ ê²€ìƒ‰ì–´ê°€ ìˆëŠ” ê²½ìš° (AND/OR/NOT ì—°ì‚°)
        if params.additional_queries:
            for idx, add_query in enumerate(params.additional_queries):
                url_params.append((f'b{idx}', add_query.operator.value))
                url_params.append((f'weight{idx}', ''))
                url_params.append(('si', add_query.search_field.value))
                url_params.append(('q', add_query.query))
            
            # ë§ˆì§€ë§‰ weight íŒŒë¼ë¯¸í„°
            last_weight_idx = len(params.additional_queries)
            url_params.append((f'weight{last_weight_idx}', ''))
        
        # ìë£Œìœ í˜• íŒŒë¼ë¯¸í„°
        material_type_values = [mt.value for mt in params.material_types]
        material_type_order = ['TOTAL', 'm', 's', 'b;p;v;x;u;c', 't', 'o', 'zart']
        
        # ì²« ë²ˆì§¸ _lmt0 (í•­ìƒ on)
        url_params.append(('_lmt0', 'on'))
        url_params.append(('lmtsn', '000000000001'))
        url_params.append(('lmtst', 'OR'))
        
        # ì„ íƒëœ ìë£Œìœ í˜•ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¶”ê°€
        for mat_type in material_type_order:
            url_params.append(('_lmt0', 'on'))
            if mat_type in material_type_values:
                url_params.append(('lmt0', mat_type))
        
        # ìˆ˜ë¡ë§¤ì²´ ì œí•œ (inc)
        url_params.append(('inc', 'TOTAL'))
        for _ in range(6):
            url_params.append(('_inc', 'on'))
        
        # ì–¸ì–´ ì œí•œ (lmt1)
        url_params.append(('lmt1', 'TOTAL'))
        url_params.append(('lmtsn', '000000000003'))
        url_params.append(('lmtst', 'OR'))
        
        # ì†Œì¥ì²˜ ì œí•œ (lmt2) - ì‹ ì´Œ+êµ­ì œ
        url_params.append(('lmt2', 'YNLIB;GSISL;MUSEL;OTHER;UGSTL;YSLIB;ARCHL;BUSIL;KORCL;IOKSL;LAWSL;MULTL;MATHL;MUSIC;UML'))
        url_params.append(('lmtsn', '000000000006'))
        url_params.append(('lmtst', 'OR'))
        
        # ë°œí–‰ë…„ë„ ë²”ìœ„ ì„¤ì •
        if params.year_range:
            if params.year_range.from_year:
                url_params.append(('rf', str(params.year_range.from_year)))
            if params.year_range.to_year:
                url_params.append(('rt', str(params.year_range.to_year)))
            if params.year_range.from_year or params.year_range.to_year:
                url_params.append(('range', '000000000021'))
        
        # í˜ì´ì§• ì„¤ì •
        url_params.append(('cpp', str(params.results_per_page)))  # ìª½ë‹¹ ì¶œë ¥ ê±´ìˆ˜
        url_params.append(('msc', '10000'))  # ìµœëŒ€ ê²€ìƒ‰ ê±´ìˆ˜
        
        # URL íŒŒë¼ë¯¸í„° ë¬¸ìì—´ êµ¬ì„±
        param_string = "&".join([f"{k}={quote(str(v))}" for k, v in url_params])
        
        return f"{self.base_url}{endpoint}?{param_string}"
    
    def _parse_holdings_search_results(self, html_content: str, search_type: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ í•­ëª© ì„ íƒì (ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        result_items = soup.select('.search-result-item, .list-item, .result-item')
        
        for item in result_items:
            try:
                result = self._extract_result_info(item, search_type)
                if result:
                    results.append(result)
            except Exception as e:
                logger.warning(f"Failed to parse result item: {e}")
                continue
        
        return results
    
    def _extract_result_info(self, item_element, search_type: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ"""
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = item_element.select_one('.title, .item-title, h3, h4')
            title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
            
            # ì €ì ì¶”ì¶œ
            author_elem = item_element.select_one('.author, .item-author, .creator')
            authors = []
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                authors = [author.strip() for author in author_text.split(',')]
            
            # ì¶œíŒ ì •ë³´ ì¶”ì¶œ
            pub_elem = item_element.select_one('.publication, .pub-info, .publisher')
            publication_info = pub_elem.get_text(strip=True) if pub_elem else ""
            
            # ì—°ë„ ì¶”ì¶œ
            year = self._extract_year(publication_info + " " + title)
            
            # ìƒì„¸ ë§í¬ ì¶”ì¶œ
            link_elem = item_element.select_one('a[href]')
            detail_link = ""
            if link_elem:
                href = link_elem.get('href')
                detail_link = urljoin(self.base_url, href) if href else ""
            
            # ìë£Œ ìœ í˜• ì¶”ì¶œ
            type_elem = item_element.select_one('.type, .material-type, .format')
            material_type = type_elem.get_text(strip=True) if type_elem else "ê¸°íƒ€"
            
            return {
                "title": title,
                "authors": authors,
                "publication_info": publication_info,
                "year": year,
                "material_type": material_type,
                "detail_link": detail_link,
                "search_type": search_type
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract result info: {e}")
            return None
    
    async def _get_detailed_info(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        
        if not result.get('detail_link'):
            return result
        
        try:
            response = self.session.get(result['detail_link'], timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì´ˆë¡ ì¶”ì¶œ
            abstract_elem = soup.select_one('.abstract, .summary, .description')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_elem = soup.select_one('.keywords, .subjects, .tags')
            keywords = []
            if keywords_elem:
                keyword_text = keywords_elem.get_text(strip=True)
                keywords = [kw.strip() for kw in keyword_text.split(',')]
            
            # ì†Œì¥ ì •ë³´ ì¶”ì¶œ
            holdings = self._extract_holdings_info(soup)
            
            # ì›ë¬¸ ë§í¬ ì¶”ì¶œ
            fulltext_elem = soup.select_one('.fulltext-link, .pdf-link, .online-access')
            fulltext_link = ""
            if fulltext_elem:
                href = fulltext_elem.get('href')
                fulltext_link = urljoin(self.base_url, href) if href else ""
            
            # ê¸°ì¡´ ê²°ê³¼ì— ìƒì„¸ ì •ë³´ ì¶”ê°€
            result.update({
                "abstract": abstract[:500] + "..." if len(abstract) > 500 else abstract,
                "keywords": keywords,
                "holdings": holdings,
                "fulltext_link": fulltext_link
            })
            
            return result
            
        except Exception as e:
            logger.warning(f"Failed to get detailed info: {e}")
            return result
    
    async def _get_holdings_detail(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
        
        holdings = result.get('holdings', {})
        
        # ê¸°ë³¸ ì†Œì¥ ì •ë³´ê°€ ì—†ìœ¼ë©´ Mock ë°ì´í„° ìƒì„±
        if not holdings:
            return self._generate_mock_holdings()
        
        return holdings
    
    def _extract_holdings_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì†Œì¥ ì •ë³´ ì¶”ì¶œ"""
        
        holdings = {
            "locations": [],
            "status": "available",
            "loan_status": "ëŒ€ì¶œ ê°€ëŠ¥",
            "access_type": "physical"
        }
        
        # ì†Œì¥ ìœ„ì¹˜ ì¶”ì¶œ
        location_elems = soup.select('.location, .library-location, .holdings-location')
        for loc_elem in location_elems:
            location_text = loc_elem.get_text(strip=True)
            if location_text:
                holdings["locations"].append(location_text)
        
        # ëŒ€ì¶œ ìƒíƒœ ì¶”ì¶œ
        status_elem = soup.select_one('.status, .availability, .loan-status')
        if status_elem:
            status_text = status_elem.get_text(strip=True)
            holdings["loan_status"] = status_text
            
            # ìƒíƒœì— ë”°ë¥¸ ê°€ìš©ì„± íŒë‹¨
            if any(keyword in status_text for keyword in ["ëŒ€ì¶œì¤‘", "ì´ìš©ë¶ˆê°€", "ë¶„ì‹¤"]):
                holdings["status"] = "unavailable"
        
        # ì˜¨ë¼ì¸ ì ‘ê·¼ ì—¬ë¶€ í™•ì¸
        online_elem = soup.select_one('.online-access, .electronic-resource, .e-resource')
        if online_elem:
            holdings["access_type"] = "electronic"
            holdings["loan_status"] = "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        
        return holdings
    
    def _generate_access_info(self, holdings: Dict[str, Any]) -> str:
        """ì ‘ê·¼ ì •ë³´ ìƒì„±"""
        
        access_type = holdings.get("access_type", "physical")
        locations = holdings.get("locations", [])
        loan_status = holdings.get("loan_status", "")
        
        if access_type == "electronic":
            return "âœ… ì „ì ì €ë„ ì›ë¬¸ ì´ìš© ê°€ëŠ¥"
        elif locations:
            location_str = ", ".join(locations[:2])  # ìµœëŒ€ 2ê°œ ìœ„ì¹˜ë§Œ í‘œì‹œ
            return f"ğŸ“š {location_str} - {loan_status}"
        else:
            return f"ğŸ“– {loan_status}"
    
    def _generate_mock_holdings(self) -> Dict[str, Any]:
        """Mock ì†Œì¥ ì •ë³´ ìƒì„±"""
        import random
        
        mock_locations = [
            "ì¤‘ì•™ë„ì„œê´€ 3ì¸µ",
            "í•™ìˆ ì •ë³´ì› 2ì¸µ", 
            "ê³¼í•™ë„ì„œê´€ 1ì¸µ",
            "ì˜í•™ë„ì„œê´€"
        ]
        
        mock_statuses = [
            "ëŒ€ì¶œ ê°€ëŠ¥",
            "ëŒ€ì¶œì¤‘",
            "ì˜¨ë¼ì¸ ì´ìš© ê°€ëŠ¥"
        ]
        
        return {
            "locations": [random.choice(mock_locations)],
            "status": "available",
            "loan_status": random.choice(mock_statuses),
            "access_type": random.choice(["physical", "electronic"])
        }
    
    def _extract_year(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        import re
        
        # 4ìë¦¬ ì—°ë„ íŒ¨í„´ ì°¾ê¸°
        year_pattern = r'\b(19|20)\d{2}\b'
        matches = re.findall(year_pattern, text)
        
        if matches:
            # ê°€ì¥ ìµœê·¼ ì—°ë„ ë°˜í™˜
            years = [int(match + m[2:]) for match, m in re.findall(r'\b(19|20)(\d{2})\b', text)]
            return max(years) if years else 0
        
        return 0
    
    def __del__(self):
        """ì†Œë©¸ì: ì„¸ì…˜ ì •ë¦¬"""
        if hasattr(self, 'session'):
            self.session.close()