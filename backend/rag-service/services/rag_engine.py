from typing import List, Dict, Any
from shared.models import SearchStrategy, RAGAnalysisResponse, DocumentResult
from .vector_store import VectorStoreManager
from .document_processor import DocumentProcessor
import logging

logger = logging.getLogger(__name__)

class RAGEngine:
    """RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì—”ì§„ì˜ í•µì‹¬ ë¡œì§"""
    
    def __init__(self, vector_store: VectorStoreManager, document_processor: DocumentProcessor):
        self.vector_store = vector_store
        self.document_processor = document_processor
        self.analysis_cache = {}  # ë¶„ì„ ê²°ê³¼ ìºì‹±
    
    async def analyze_documents(
        self,
        search_strategy: SearchStrategy,
        session_id: str,
        analysis_depth: str = "standard"
    ) -> RAGAnalysisResponse:
        """ê²€ìƒ‰ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ë¶„ì„ ìˆ˜í–‰"""
        
        try:
            # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            relevant_documents = await self._retrieve_relevant_documents(search_strategy)
            
            # 2. ë¬¸ì„œ ë¶„ì„
            analysis_results = await self._analyze_retrieved_documents(
                relevant_documents, search_strategy, analysis_depth
            )
            
            # 3. í•µì‹¬ ë…¼ìŸ ì§€ì  ì¶”ì¶œ
            key_debates = await self._extract_key_debates(analysis_results)
            
            # 4. ë¬¸í—Œ ë©”íƒ€ë°ì´í„° ìƒì„±
            document_metadata = await self._generate_document_metadata(relevant_documents)
            
            # 5. ë¶„ì„ ì‹ ë¢°ë„ í‰ê°€
            confidence_score = self._calculate_analysis_confidence(
                len(relevant_documents), analysis_results
            )
            
            return RAGAnalysisResponse(
                session_id=session_id,
                analysis_summary=analysis_results["summary"],
                key_debates=key_debates,
                relevant_documents=document_metadata,
                confidence_score=confidence_score,
                search_strategy_used=search_strategy,
                analysis_limitations=self._generate_limitations_note(relevant_documents)
            )
        
        except Exception as e:
            logger.error(f"RAG analysis failed: {e}")
            raise
    
    async def _retrieve_relevant_documents(self, search_strategy: SearchStrategy) -> List[Dict]:
        """ê²€ìƒ‰ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        
        # ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¡°í•©í•˜ì—¬ ì¿¼ë¦¬ ìƒì„±
        query_text = " ".join(search_strategy.primary_keywords)
        
        # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        similar_documents = await self.vector_store.similarity_search(
            query=query_text,
            limit=50,  # ì¶©ë¶„í•œ ìˆ˜ì˜ ë¬¸ì„œ ê²€ìƒ‰
            filter_metadata={"academic_field": search_strategy.academic_fields}
        )
        
        # í™•ì¥ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
        if search_strategy.expansion_keywords:
            expansion_query = " ".join(search_strategy.expansion_keywords[:3])
            additional_docs = await self.vector_store.similarity_search(
                query=expansion_query,
                limit=20
            )
            similar_documents.extend(additional_docs)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        unique_docs = self._deduplicate_documents(similar_documents)
        return sorted(unique_docs, key=lambda x: x.get('relevance_score', 0), reverse=True)[:20]
    
    async def _analyze_retrieved_documents(
        self,
        documents: List[Dict],
        search_strategy: SearchStrategy,
        depth: str
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì— ëŒ€í•œ ë¶„ì„ ìˆ˜í–‰"""
        
        analysis = {
            "summary": "",
            "themes": [],
            "methodologies": [],
            "findings": [],
            "gaps": []
        }
        
        if depth == "detailed":
            # ìƒì„¸ ë¶„ì„ ë¡œì§
            analysis = await self._perform_detailed_analysis(documents, search_strategy)
        else:
            # í‘œì¤€ ë¶„ì„ ë¡œì§
            analysis = await self._perform_standard_analysis(documents, search_strategy)
        
        return analysis
    
    async def _perform_standard_analysis(
        self,
        documents: List[Dict],
        search_strategy: SearchStrategy
    ) -> Dict[str, Any]:
        """í‘œì¤€ ìˆ˜ì¤€ì˜ ë¬¸ì„œ ë¶„ì„"""
        
        # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        document_contents = [doc.get('content', '') for doc in documents]
        
        # ì£¼ìš” í…Œë§ˆ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)
        themes = self._extract_themes(document_contents, search_strategy.primary_keywords)
        
        # ì—°êµ¬ ë°©ë²•ë¡  ë¶„ì„
        methodologies = self._analyze_methodologies(document_contents)
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­ ìš”ì•½
        findings = self._summarize_findings(document_contents)
        
        # ì—°êµ¬ ê³µë°± ì‹ë³„
        gaps = self._identify_research_gaps(document_contents, search_strategy)
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        summary = self._generate_analysis_summary(themes, methodologies, findings)
        
        return {
            "summary": summary,
            "themes": themes,
            "methodologies": methodologies,
            "findings": findings,
            "gaps": gaps
        }
    
    async def _perform_detailed_analysis(
        self,
        documents: List[Dict],
        search_strategy: SearchStrategy
    ) -> Dict[str, Any]:
        """ìƒì„¸ ìˆ˜ì¤€ì˜ ë¬¸ì„œ ë¶„ì„ (LLM í™œìš©)"""
        
        # TODO: LLMì„ í™œìš©í•œ ë” ì •êµí•œ ë¶„ì„ ë¡œì§
        # í˜„ì¬ëŠ” í‘œì¤€ ë¶„ì„ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        return await self._perform_standard_analysis(documents, search_strategy)
    
    async def _extract_key_debates(self, analysis_results: Dict[str, Any]) -> List[Dict[str, str]]:
        """í•µì‹¬ ë…¼ìŸ ì§€ì  ì¶”ì¶œ"""
        
        debates = []
        
        # í…Œë§ˆì—ì„œ ìƒë°˜ëœ ê´€ì  ì°¾ê¸°
        for theme in analysis_results.get("themes", []):
            if self._contains_contrasting_views(theme):
                debates.append({
                    "topic": theme["name"],
                    "description": theme["controversy"],
                    "positions": theme.get("different_positions", [])
                })
        
        # ì—°êµ¬ ë°©ë²•ë¡ ì˜ ì°¨ì´ì 
        methodologies = analysis_results.get("methodologies", [])
        if len(set(methodologies)) > 1:
            debates.append({
                "topic": "ì—°êµ¬ ë°©ë²•ë¡ ì  ì ‘ê·¼",
                "description": "ì—°êµ¬ìë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ë¡ ì  ì ‘ê·¼ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "positions": list(set(methodologies))
            })
        
        return debates[:5]  # ìµœëŒ€ 5ê°œì˜ ì£¼ìš” ë…¼ìŸì 
    
    async def _generate_document_metadata(self, documents: List[Dict]) -> List[DocumentResult]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        
        metadata_list = []
        
        for doc in documents[:10]:  # ìƒìœ„ 10ê°œ ë¬¸ì„œë§Œ
            # ì—°ì„¸ëŒ€ ë„ì„œê´€ ì†Œì¥ ì—¬ë¶€ í™•ì¸ (Mock)
            availability = await self._check_yonsei_library_availability(doc)
            
            metadata = DocumentResult(
                title=doc.get('title', 'Unknown Title'),
                authors=doc.get('authors', []),
                publication_year=doc.get('year', 0),
                source=doc.get('source', 'Unknown Source'),
                abstract=doc.get('abstract', '')[:200] + "...",
                relevance_score=doc.get('relevance_score', 0.0),
                yonsei_library_status=availability,
                access_link=doc.get('access_link', '')
            )
            
            metadata_list.append(metadata)
        
        return metadata_list
    
    async def _check_yonsei_library_availability(self, document: Dict) -> str:
        """ì—°ì„¸ëŒ€ ë„ì„œê´€ ì†Œì¥ ì—¬ë¶€ í™•ì¸ (Mock êµ¬í˜„)"""
        # TODO: ì‹¤ì œ ë„ì„œê´€ API ì—°ë™
        import random
        statuses = [
            "âœ… ì „ì ì €ë„ ì›ë¬¸ ì´ìš© ê°€ëŠ¥",
            "ğŸ“š ì¤‘ì•™ë„ì„œê´€ ëŒ€ì¶œ ê°€ëŠ¥",
            "ğŸ“œ dCollectionì—ì„œ ì›ë¬¸ ì´ìš© ê°€ëŠ¥",
            "âŒ ì†Œì¥í•˜ì§€ ì•ŠìŒ"
        ]
        return random.choice(statuses)
    
    def _calculate_analysis_confidence(self, doc_count: int, analysis: Dict) -> float:
        """ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.6
        
        # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        doc_weight = min(doc_count * 0.02, 0.3)
        
        # ë¶„ì„ ê²°ê³¼ ì™„ì„±ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        completeness_weight = len(analysis.get("themes", [])) * 0.05
        
        return min(base_confidence + doc_weight + completeness_weight, 0.95)
    
    def _generate_limitations_note(self, documents: List[Dict]) -> str:
        """ë¶„ì„ ì œí•œì‚¬í•­ ì•ˆë‚´ë¬¸ ìƒì„±"""
        return f"""
        ì´ ë¶„ì„ì€ ì œí•œëœ ìƒ˜í”Œ ë°ì´í„°ì…‹({len(documents)}ê°œ ë¬¸ì„œ)ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜ˆë¹„ ë¶„ì„ì…ë‹ˆë‹¤.
        
        ì£¼ìš” ì œí•œì‚¬í•­:
        1. ì „ì²´ í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ê°€ ì•„ë‹Œ ë¶€ë¶„ì  ìƒ˜í”Œ ê¸°ë°˜
        2. ìµœì‹  ì—°êµ¬ ë™í–¥ì´ ì™„ì „íˆ ë°˜ì˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        3. ì–¸ì–´ì  í¸í–¥ ê°€ëŠ¥ì„± (í•œêµ­ì–´/ì˜ì–´ ë¬¸í—Œ ìœ„ì£¼)
        
        ë³´ë‹¤ í¬ê´„ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ì„œëŠ” ì‹¤ì œ ë„ì„œê´€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
        """
    
    # í—¬í¼ ë©”ì„œë“œë“¤
    def _deduplicate_documents(self, documents: List[Dict]) -> List[Dict]:
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_docs = []
        
        for doc in documents:
            title = doc.get('title', '')
            if title not in seen_titles:
                seen_titles.add(title)
                unique_docs.append(doc)
        
        return unique_docs
    
    def _extract_themes(self, contents: List[str], keywords: List[str]) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ í…Œë§ˆ ì¶”ì¶œ
        themes = []
        for keyword in keywords:
            mention_count = sum(1 for content in contents if keyword in content)
            if mention_count > 2:
                themes.append({
                    "name": keyword,
                    "frequency": mention_count,
                    "description": f"'{keyword}' ê´€ë ¨ ì—°êµ¬ ë™í–¥"
                })
        
        return themes
    
    def _analyze_methodologies(self, contents: List[str]) -> List[str]:
        """ì—°êµ¬ ë°©ë²•ë¡  ë¶„ì„"""
        methodologies = set()
        method_keywords = {
            "ì„¤ë¬¸ì¡°ì‚¬": ["ì„¤ë¬¸", "survey", "questionnaire"],
            "ì‹¤í—˜ì—°êµ¬": ["ì‹¤í—˜", "experiment", "ì‹¤í—˜êµ°"],
            "ì§ˆì ì—°êµ¬": ["ì¸í„°ë·°", "interview", "ì§ˆì "],
            "ì–‘ì ì—°êµ¬": ["í†µê³„", "íšŒê·€ë¶„ì„", "ìƒê´€ë¶„ì„"],
            "ë¬¸í—Œì—°êµ¬": ["ë¬¸í—Œ", "ë¦¬ë·°", "ë©”íƒ€ë¶„ì„"]
        }
        
        for content in contents:
            for method, keywords in method_keywords.items():
                if any(kw in content for kw in keywords):
                    methodologies.add(method)
        
        return list(methodologies)
    
    def _summarize_findings(self, contents: List[str]) -> List[str]:
        """ì£¼ìš” ë°œê²¬ì‚¬í•­ ìš”ì•½"""
        # ê°„ë‹¨í•œ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ ë¡œì§
        findings = []
        finding_indicators = ["ê²°ê³¼", "ë°œê²¬", "ë‚˜íƒ€ë‚¬ë‹¤", "í™•ì¸ë˜ì—ˆë‹¤"]
        
        for content in contents:
            sentences = content.split('.')
            for sentence in sentences:
                if any(indicator in sentence for indicator in finding_indicators):
                    findings.append(sentence.strip())
                    if len(findings) >= 5:
                        break
        
        return findings[:5]
    
    def _identify_research_gaps(self, contents: List[str], strategy: SearchStrategy) -> List[str]:
        """ì—°êµ¬ ê³µë°± ì‹ë³„"""
        gaps = []
        
        # í‚¤ì›Œë“œ ì¡°í•©ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„ ì°¾ê¸°
        all_keywords = strategy.primary_keywords + strategy.expansion_keywords
        
        # ê°„ë‹¨í•œ ê³µë°± ì‹ë³„ ë¡œì§
        if len(all_keywords) > 2:
            gaps.append("í‚¤ì›Œë“œ ê°„ ìƒí˜¸ì‘ìš© íš¨ê³¼ì— ëŒ€í•œ ì—°êµ¬ ë¶€ì¡±")
        
        gaps.append("í•œêµ­ì  ë§¥ë½ì—ì„œì˜ ì—°êµ¬ í•„ìš”")
        gaps.append("ìµœì‹  ê¸°ìˆ  ë™í–¥ ë°˜ì˜ ì—°êµ¬ ë¶€ì¡±")
        
        return gaps[:3]
    
    def _generate_analysis_summary(self, themes: List[Dict], methodologies: List[str], findings: List[str]) -> str:
        """ì „ì²´ ë¶„ì„ ìš”ì•½ ìƒì„±"""
        theme_names = [theme["name"] for theme in themes]
        
        summary = f"""
        ê²€ìƒ‰ëœ ë¬¸í—Œë“¤ì„ ë¶„ì„í•œ ê²°ê³¼, ì£¼ìš” ì—°êµ¬ ì£¼ì œëŠ” {', '.join(theme_names[:3])} ë“±ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
        
        ì—°êµ¬ ë°©ë²•ë¡ ì ìœ¼ë¡œëŠ” {', '.join(methodologies[:3])} ë“±ì˜ ì ‘ê·¼ì´ ì£¼ë¡œ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°,
        
        ì£¼ìš” ë°œê²¬ì‚¬í•­ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ë“¤ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤:
        {chr(10).join(['- ' + finding[:100] + '...' for finding in findings[:3]])}
        """
        
        return summary.strip()
    
    def _contains_contrasting_views(self, theme: Dict) -> bool:
        """í…Œë§ˆì— ìƒë°˜ëœ ê´€ì ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ ë…¼ìŸ íŒë‹¨ ë¡œì§
        controversial_keywords = ["ë°˜ë©´", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ëŒ€ì¡°ì ìœ¼ë¡œ", "ìƒë°˜ëœ"]
        description = theme.get("description", "")
        
        return any(kw in description for kw in controversial_keywords)
    
    async def ingest_documents(self, documents: List[Dict]) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ë¬¸ì„œë“¤ì„ ì‹œìŠ¤í…œì— ì¶”ê°€"""
        
        processed_count = 0
        
        for doc in documents:
            try:
                # ë¬¸ì„œ ì „ì²˜ë¦¬
                processed_doc = await self.document_processor.process_document(doc)
                
                # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                await self.vector_store.add_document(processed_doc)
                
                processed_count += 1
                
            except Exception as e:
                logger.error(f"Failed to ingest document {doc.get('title', 'Unknown')}: {e}")
        
        return {"count": processed_count}