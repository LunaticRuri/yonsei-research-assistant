# íŒŒì¼ëª…: run_poc_test.py
# (backend/strategy-service í´ë”ì— ì €ì¥)

import os
import time
import openai
import anthropic
from dotenv import load_dotenv

# --- 1. í™˜ê²½ ì„¤ì • ---
print("INFO: .env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
load_dotenv()

try:
    openai_client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    anthropic_client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])
    print("INFO: API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
except KeyError:
    print("ì˜¤ë¥˜: .env íŒŒì¼ì— OPENAI_API_KEY ë˜ëŠ” ANTHROPIC_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --- 2. v8.0 ê¸°íšì•ˆ í•µì‹¬ ìš”ì†Œ ì •ì˜ ---

# 2-1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (v8.0 ê¸°íšì•ˆì˜ 'ìˆ˜ì œ ë¶ˆë¦¬ì–¸ ë¬¸ì œ' ì˜ˆì‹œ)
# OOë‹˜ì´ ì´ ë¶€ë¶„ì„ ììœ ë¡­ê²Œ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”.
SCENARIO = "ê¹€ì˜í¬ ì €ìì˜ 2023ë…„ ì´í›„ 'AI' ë…¼ë¬¸, ë‹¨ í•œêµ­ì–´ ë…¼ë¬¸ì€ ì œì™¸"
GROUND_TRUTH = '(Author = "ê¹€ì˜í¬") AND (PublicationYear >= 2023) AND (Keyword = "AI") AND (NOT Language = "Korean")'

# 2-2. í”„ë¡¬í”„íŠ¸ (v8.0 ê¸°íšì•ˆì˜ Prompt A, C)
PROMPT_A_TEMPLATE = """
ë„ˆëŠ” ì—°ì„¸ëŒ€ ë„ì„œê´€ ì‚¬ì„œì•¼. ë‹¤ìŒ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¶ˆë¦¬ì–¸ ì¿¼ë¦¬ë¡œ ë°”ê¿”ì¤˜:
{scenario}
"""

PROMPT_C_TEMPLATE = """
ë„ˆëŠ” ì—°ì„¸ëŒ€ ë„ì„œê´€ ê²€ìƒ‰ ì „ë¬¸ê°€ë‹¤.
ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œëŠ” [Author, Keyword, PublicationYear, Language] ë¿ì´ë‹¤.
ì´ ìŠ¤í‚¤ë§ˆë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹¤ìŒ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¶ˆë¦¬ì–¸ ì¿¼ë¦¬ë¡œ ë°”ê¿”ì¤˜:
{scenario}
"""

# --- 3. ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜ ---

def get_gpt4o_response(prompt):
    """Tier 1: GPT-4o í˜¸ì¶œ"""
    start_time = time.time()
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        text = response.choices[0].message.content
        latency = time.time() - start_time
        return text, f"{latency:.2f}ì´ˆ"
    except Exception as e:
        return f"GPT-4o ì˜¤ë¥˜: {e}", "N/A"

def get_haiku_response(prompt):
    """Tier 2: Claude 3 Haiku í˜¸ì¶œ"""
    start_time = time.time()
    try:
        response = anthropic_client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        text = response.content[0].text
        latency = time.time() - start_time
        return text, f"{latency:.2f}ì´ˆ"
    except Exception as e:
        return f"Haiku ì˜¤ë¥˜: {e}", "N/A"

# --- 4. PoC í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥ ---

def run_poc():
    print("=" * 50)
    print("ğŸš€ v8.0 ê¸°íšì•ˆ PoC í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {SCENARIO}")
    print(f"   (ì •ë‹µ ì¿¼ë¦¬): {GROUND_TRUTH}")
    print("=" * 50 + "\n")

    # 2x2 ë§¤íŠ¸ë¦­ìŠ¤ í…ŒìŠ¤íŠ¸
    tests_to_run = [
        ("GPT-4o (Tier 1)", "A (Zero-Shot)", PROMPT_A_TEMPLATE.format(scenario=SCENARIO), get_gpt4o_response),
        ("GPT-4o (Tier 1)", "C (Schema)", PROMPT_C_TEMPLATE.format(scenario=SCENARIO), get_gpt4o_response),
        ("Haiku (Tier 2)", "A (Zero-Shot)", PROMPT_A_TEMPLATE.format(scenario=SCENARIO), get_haiku_response),
        ("Haiku (Tier 2)", "C (Schema)", PROMPT_C_TEMPLATE.format(scenario=SCENARIO), get_haiku_response),
    ]

    results_for_demo = []

    for model_name, prompt_name, final_prompt, model_function in tests_to_run:
        print(f"--- [ì‹¤í–‰ ì¤‘: {model_name} + {prompt_name}] ---")
        response, latency = model_function(final_prompt)
        
        print(f"ì‘ë‹µ ì†ë„: {latency}")
        print(f"ëª¨ë¸ ì‘ë‹µ:\n{response}\n")
        
        # ì‹œì—°ìš© ë°ì´í„° ìˆ˜ì§‘
        results_for_demo.append((model_name, prompt_name, response, latency))

    return results_for_demo

if __name__ == "__main__":
    run_poc()