import requests
import pandas as pd
import json
import os
import time
from tqdm import tqdm

# ======================================================
# âš™ï¸ ì‹¤í—˜ ì„¤ì •
# ======================================================
SERVER_URL = "http://localhost:8000/api/v1/strategy/keywords"
BENCHMARK_FILE = "benchmark_set_20.json"
OUTPUT_FILE = "ab_test_final_report.csv"

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
MODELS_TO_TEST = ["openai", "gemini", "upstage", "cohere", "lora"]

# ======================================================
# ğŸ“¥ ë°ì´í„° ì¤€ë¹„
# ======================================================
if not os.path.exists(BENCHMARK_FILE):
    print("â¬‡ï¸ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    try:
        url = "https://raw.githubusercontent.com/LunaticRuri/yonsei-research-assistant/main/benchmark_set_20.json"
        r = requests.get(url)
        r.raise_for_status()
        with open(BENCHMARK_FILE, 'wb') as f:
            f.write(r.content)
    except:
        with open(BENCHMARK_FILE, 'w', encoding='utf-8') as f:
            json.dump([{"question": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸", "keyphrases": ["í…ŒìŠ¤íŠ¸", "í‚¤ì›Œë“œ"]}], f)

with open(BENCHMARK_FILE, "r", encoding="utf-8") as f:
    questions = json.load(f)

print(f"ğŸš€ ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì´ {len(questions)}ê°œ ë¬¸í•­)")
print(f"ğŸ¥Š ëª¨ë¸ ëª©ë¡: {MODELS_TO_TEST}")
print(f"ğŸ¯ íƒ€ê²Ÿ ì„œë²„: {SERVER_URL}\n")

# ======================================================
# ğŸ§ª ì‹¤í—˜ í•¨ìˆ˜
# ======================================================
def test_model_request(query, mode):
    try:
        response = requests.post(SERVER_URL, json={"query": query, "mode": mode}, timeout=60)
        
        if response.status_code == 200:
            data = response.json()
            strat = data.get('strategy_result', {})
            retrieval = data.get('retrieval_result', {})
            
            keywords = strat.get('keywords', '')
            latency = strat.get('latency_ms', 0)
            
            # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (í†µí•© í…ŒìŠ¤íŠ¸ìš©)
            docs = retrieval.get('documents', [])
            doc_count = len(docs) if isinstance(docs, list) else 0
            
            return keywords, latency, doc_count
        else:
            return f"HTTP Error {response.status_code}", 0, 0
    except Exception as e:
        return f"Conn Error", 0, 0

# ======================================================
# ğŸ”„ ì‹¤í—˜ ì‹¤í–‰
# ======================================================
results = []

for idx, item in enumerate(tqdm(questions)):
    # 1. ë°ì´í„° íŒŒì‹±
    query = item.get('question', item.get('query'))
    
    # [New] ì •ë‹µ í‚¤ì›Œë“œ (K-RAG ë°ì´í„°ì…‹ì— ìˆëŠ” ê²½ìš° ê°€ì ¸ì˜´)
    ground_truth = item.get('keyphrases', [])
    if isinstance(ground_truth, list):
        ground_truth_str = ", ".join(map(str, ground_truth))
    else:
        ground_truth_str = str(ground_truth)

    # 2. ê²°ê³¼ í–‰ ì´ˆê¸°í™”
    row = {
        "ID": idx + 1,
        "Question": query,
        "Ground_Truth": ground_truth_str # ğŸ‘ˆ ì •ë‹µì§€ ì»¬ëŸ¼ ì¶”ê°€!
    }
    
    fastest_time = float('inf')
    fastest_model = "None"

    # 3. ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸
    for model_name in MODELS_TO_TEST:
        k, t, d = test_model_request(query, model_name)
        
        row[f"{model_name}_Keywords"] = k
        row[f"{model_name}_Latency"] = t
        row[f"{model_name}_Docs"] = d

        # [New] ê¸€ì ìˆ˜ ì¶”ê°€ (ë¹„ìš© ê³„ì‚°ìš© ê·¼ê±° ë°ì´í„°)
        row[f"{model_name}_Len"] = len(str(k))
        
        if t > 0 and t < fastest_time:
            fastest_time = t
            fastest_model = model_name
            
        time.sleep(0.1)

    row["Fastest_Model"] = fastest_model
    results.append(row)

# ======================================================
# ğŸ’¾ ì €ì¥ ë° í†µê³„
# ======================================================
df = pd.DataFrame(results)
df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

print("\\n" + "="*50)
print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_FILE}")
print("ğŸ“Š [ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½]")

for model_name in MODELS_TO_TEST:
    lat_col = f"{model_name}_Latency"
    doc_col = f"{model_name}_Docs"
    
    if lat_col in df.columns:
        avg_time = df[df[lat_col] > 0][lat_col].mean()
        
        # ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨ (ë¬¸ì„œ 0ê±´ ë¹„ìœ¨)
        total_runs = len(df)
        failed_runs = len(df[df[doc_col] == 0])
        fail_rate = (failed_runs / total_runs) * 100
        
        print(f"ğŸ“Œ [{model_name}]")
        print(f"   - í‰ê·  ì†ë„: {avg_time:.2f} ms")
        print(f"   - ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨: {fail_rate:.1f}% ({failed_runs}/{total_runs}ê±´)")
        print("-" * 30)

print("="*50)
