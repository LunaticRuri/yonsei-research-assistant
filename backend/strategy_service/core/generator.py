import os
import time
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel
from langchain_core.prompts import ChatPromptTemplate
import re

# ë¶€í’ˆë“¤ ê°€ì ¸ì˜¤ê¸°
from strategy_service.core.providers.openai_handler import OpenAIHandler
from strategy_service.core.providers.gemini_handler import GeminiHandler

# NOTE: ì•„ë˜ ë‘ í•¸ë“¤ëŸ¬ëŠ” í˜„ì¬ ì£¼ì„ ì²˜ë¦¬ ìƒíƒœ
# from strategy_service.core.providers.cohere_handler import CohereHandler
# from strategy_service.core.providers.upstage_handler import UpstageHandler

from shared.models import StrategyServiceMode
from shared.config import settings

import logging


class QueryTranslationService:
    def __init__(self, adapter_path: str = None):
        print("[Init] QueryTranslationService (Factory Mode) ì´ˆê¸°í™”...")
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.logger.addHandler(settings.console_handler)
        self.logger.addHandler(settings.file_handler)

        # 1. API í•¸ë“¤ëŸ¬ ë“±ë¡ (í™•ì¥ì„± í¬ì¸íŠ¸!)
        self.api_providers = {
            "openai": OpenAIHandler(settings.OPENAI_API_KEY),
            "gemini": GeminiHandler(settings.GEMINI_API_KEY)
        }
        
        """
        ì›ë˜ ì½”ë“œ:
        # 1. API í•¸ë“¤ëŸ¬ ë“±ë¡ (í™•ì¥ì„± í¬ì¸íŠ¸!)
        self.api_providers = {
            "openai": OpenAIHandler(os.getenv("OPENAI_API_KEY")),
            "gemini": GeminiHandler(os.getenv("GEMINI_API_KEY")), 
            "upstage": UpstageHandler(os.getenv("UPSTAGE_API_KEY")),
            "cohere": CohereHandler(os.getenv("COHERE_API_KEY"))
        }
        """

        # 2. LoRA ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        self.lora_model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        if adapter_path and os.path.exists(adapter_path):
            try:
                base_model_id = "paust/pko-flan-t5-large"
                self.logger.info(f"ğŸ”„ LoRA ëª¨ë¸ ë¡œë“œ ì‹œë„: {adapter_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
                base_model = AutoModelForSeq2SeqLM.from_pretrained(
                    base_model_id, 
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map=self.device
                )
                self.lora_model = PeftModel.from_pretrained(base_model, adapter_path)
                self.lora_model.eval()
                self.logger.info("âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                self.logger.error(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ({adapter_path}). LoRAëŠ” [Mock] ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    async def _generate_by_lora(self, query):
        
        if self.lora_model is None:
            await asyncio.sleep(0.5) 
            return f"[Mock] '{query}'ì— ëŒ€í•œ ë¡œì»¬ í‚¤ì›Œë“œ (ëª¨ë¸ ë¯¸ì—°ê²°)"

        def text_cleaning(text):
            if not isinstance(text, str):
                return ""

            text = text.replace('\n', ' ')
            text = re.sub(r'[^ê°€-í£a-zA-Z0-9 :,]', ',', text)
            if ":" in text:
                first, rest = text.split(":", 1)
                rest = rest.replace(":", ",")
                text = first + ":" + rest

            no_words=['í˜¹ì€', 'ë°',' ë“±', 'ë˜ëŠ”', 'ì— ëŒ€í•œ', 'ì— ëŒ€í•´', 'ì— ê´€í•œ', 'ì— ê´€í•´', 'ê´€ë ¨']
            for word in no_words:
                text = text.replace(word, ',')
            
            text = re.sub(r'\s*,+\s*', ',', text)
            text = re.sub(r'(?<![ê°€-í£A-Za-z0-9]),|,(?![ê°€-í£A-Za-z0-9])', '', text)
            text = text.strip()

            return text
        
        def run_inference():
            input_text = ChatPromptTemplate.from_messages(
                [
                    ("system", "ì§€ê¸ˆë¶€í„° ë‹¹ì‹ ì€ ëŒ€í•™ í•™ìˆ  ì •ë³´ì›ì˜ ì‚¬ì„œì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì •ë³´ ì´ìš©ìê°€ ì›í•˜ëŠ” ìë£Œë¥¼ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì•¼ í•©ë‹ˆë‹¤."),
                    ("human", """### ì§ˆë¬¸: {question}\n            ì €ì˜ 'ì§ˆë¬¸'ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì œê°€ ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  'í•µì‹¬ ê²€ìƒ‰ì–´(Keywords)'ë“¤ì„ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì´ ì•„ë‹Œ ëª…ì‚¬í˜• ë‹¨ì–´ ëª©ë¡ìœ¼ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ê¸ˆì§€ì–´: 'íŠ¹ì§•', 'ê´€ë ¨', 'ì„ í–‰' ,'ì—°êµ¬', 'ë…¼ë¬¸', 'ë¬¸í—Œ'""")
                    ]
            )
            input_text = input_text.format_messages(question=query)
            input_text = "\n".join([m.content for m in input_text])
            inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).to(self.device)
            with torch.no_grad():
                outputs = self.lora_model.generate(**inputs, 
                                                   max_new_tokens=128, 
                                                   num_beams=3,
                                                   repetition_penalty=1.2, 
                                                   no_repeat_ngram_size=2,
                                                   early_stopping=True
                                                   )
                
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        decode = await asyncio.to_thread(run_inference)
        
        self.logger.debug(f"LoRA ìƒì„± ê²°ê³¼ (ì „ì²˜ë¦¬ ì „): {decode}")

        processed_output = text_cleaning(decode)

        self.logger.debug(f"LoRA ìƒì„± ê²°ê³¼ (ì „ì²˜ë¦¬ í›„): {processed_output}")
        return processed_output

    async def generate_keywords(self, query, mode: StrategyServiceMode):
        start_time = time.time()
        result = ""
        try:
            # 1. LoRA ëª¨ë“œ
            if mode == "lora":
                result = await self._generate_by_lora(query)
            
            # 2. API ëª¨ë“œ (ë™ì  ì„ íƒ)
            elif mode in self.api_providers:
                handler = self.api_providers[mode]
                result = await handler.generate_keywords(query)
            
            # 3. ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode} -> ê¸°ë³¸ê°’ ë°˜í™˜")
                raise ValueError("Unsupported mode")
            
            return {
                "query": query, 
                "mode": mode, 
                "keywords": result,
                "latency_ms": round((time.time() - start_time) * 1000, 2)
            }
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}, ëª¨ë“œ: {mode} -> ê¸°ë³¸ê°’ ë°˜í™˜")
            return {
                "query": query, 
                "mode": mode, 
                "keywords": query,
                "latency_ms": round((time.time() - start_time) * 1000, 2)
            }
