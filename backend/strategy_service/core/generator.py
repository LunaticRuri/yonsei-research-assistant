import os
import time
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# ë¶€í’ˆë“¤ ê°€ì ¸ì˜¤ê¸°
from strategy_service.core.providers.openai_handler import OpenAIHandler
from strategy_service.core.providers.gemini_handler import GeminiHandler

# NOTE: ì•„ëž˜ ë‘ í•¸ë“¤ëŸ¬ëŠ” í˜„ìž¬ ì£¼ì„ ì²˜ë¦¬ ìƒíƒœ
# from strategy_service.core.providers.cohere_handler import CohereHandler
# from strategy_service.core.providers.upstage_handler import UpstageHandler

from shared.models import StrategyServiceMode
from shared.config import settings

import logging


class QueryTranslationService:
    def __init__(self, adapter_path: str = None):
        print("[Init] QueryTranslationService (Factory Mode) ì´ˆê¸°í™”...")
        
        self.logger = logging.getLogger(__name__)
        self.logger.addHandler(settings.console_handler)
        self.logger.addHandler(settings.file_handler)

        # 1. API í•¸ë“¤ëŸ¬ ë“±ë¡ (í™•ìž¥ì„± í¬ì¸íŠ¸!)
        self.api_providers = {
            "openai": OpenAIHandler(settings.OPENAI_API_KEY),
            "gemini": GeminiHandler(settings.GEMINI_API_KEY)
        }
        
        """
        ì›ëž˜ ì½”ë“œ:
        # 1. API í•¸ë“¤ëŸ¬ ë“±ë¡ (í™•ìž¥ì„± í¬ì¸íŠ¸!)
        self.api_providers = {
            "openai": OpenAIHandler(os.getenv("OPENAI_API_KEY")),
            "gemini": GeminiHandler(os.getenv("GEMINI_API_KEY")), 
            "upstage": UpstageHandler(os.getenv("UPSTAGE_API_KEY")),
            "cohere": CohereHandler(os.getenv("COHERE_API_KEY"))
        }
        """

        # 2. LoRA ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        self.lora_model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        if adapter_path and os.path.exists(adapter_path):
            try:
                base_model_id = "paust/pko-chat-t5-large"
                self.logger.info(f"ðŸ”„ LoRA ëª¨ë¸ ë¡œë“œ ì‹œë„: {adapter_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
                base_model = AutoModelForSeq2SeqLM.from_pretrained(
                    base_model_id, 
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map=self.device
                )
                self.lora_model = PeftModel.from_pretrained(base_model, adapter_path)
                self.lora_model.eval()
                self.logger.info("âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                self.logger.error(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ({adapter_path}). LoRAëŠ” [Mock] ëª¨ë“œë¡œ ë™ìž‘í•©ë‹ˆë‹¤.")

    async def _generate_by_lora(self, query):
        
        if self.lora_model is None:
            await asyncio.sleep(0.5) 
            return f"[Mock] '{query}'ì— ëŒ€í•œ ë¡œì»¬ í‚¤ì›Œë“œ (ëª¨ë¸ ë¯¸ì—°ê²°)"
        
        def run_inference():
            input_text = f"### ì§ˆë¬¸:\n{query}\n### í•µì‹¬ ê²€ìƒ‰ì–´ ëª©ë¡:"
            inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).to(self.device)
            with torch.no_grad():
                outputs = self.lora_model.generate(**inputs, max_new_tokens=128)
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return await asyncio.to_thread(run_inference)

    async def generate_keywords(self, query, mode: StrategyServiceMode):
        start_time = time.time()
        result = ""
        try:
            # 1. LoRA ëª¨ë“œ
            if mode == "lora":
                result = await self._generate_by_lora(query)
            
            # 2. API ëª¨ë“œ (ë™ì  ì„ íƒ)
            elif mode in self.api_providers:
                handler = self.api_providers[mode]
                result = await handler.generate_keywords(query)
            
            # 3. ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode} -> ê¸°ë³¸ê°’ ë°˜í™˜")
                raise ValueError("Unsupported mode")
            
            return {
                "query": query, 
                "mode": mode, 
                "keywords": result,
                "latency_ms": round((time.time() - start_time) * 1000, 2)
            }
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}, ëª¨ë“œ: {mode} -> ê¸°ë³¸ê°’ ë°˜í™˜")
            return {
                "query": query, 
                "mode": mode, 
                "keywords": query,
                "latency_ms": round((time.time() - start_time) * 1000, 2)
            }