import os
import time
import torch
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

class QueryTranslationService:
    def __init__(self, adapter_path: str = None):
        print("âš™ï¸ [Init] QueryTranslationService ì´ˆê¸°í™” ì¤‘...")
        
        # 1. API í´ë¼ì´ì–¸íŠ¸
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.client = OpenAI(api_key=api_key)
            print("âœ… OpenAI Client ì—°ê²° ì„±ê³µ")
        else:
            self.client = None
            print("âš ï¸ OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. API ëª¨ë“œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # 2. LoRA ëª¨ë¸ (Mocking ì§€ì›)
        self.lora_model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        if adapter_path and os.path.exists(adapter_path):
            try:
                base_model_id = "paust/pko-chat-t5-large"
                print(f"ğŸ”„ LoRA ëª¨ë¸ ë¡œë“œ ì‹œë„: {adapter_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)
                base_model = AutoModelForSeq2SeqLM.from_pretrained(
                    base_model_id, 
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map=self.device
                )
                self.lora_model = PeftModel.from_pretrained(base_model, adapter_path)
                self.lora_model.eval()
                print("âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ({adapter_path}). LoRAëŠ” [Mock] ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    def _generate_by_api(self, query):
        if not self.client: return "[Error] API Key Missing"
        prompt = f"ì§ˆë¬¸: {query}\nê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ ì¶”ì¶œí•´ì¤˜." 
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"[Error] API Call Failed: {str(e)}"

    def _generate_by_lora(self, query):
        # [Mocking Logic]
        if self.lora_model is None:
            time.sleep(0.5) 
            return f"[Mock] '{query}'ì— ëŒ€í•œ ë¡œì»¬ í‚¤ì›Œë“œ (ëª¨ë¸ ë¯¸ì—°ê²°)"
            
        input_text = f"### ì§ˆë¬¸:\n{query}\n### í•µì‹¬ ê²€ìƒ‰ì–´ ëª©ë¡:"
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).to(self.device)
        with torch.no_grad():
            outputs = self.lora_model.generate(**inputs, max_new_tokens=128)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def generate_keywords(self, query, mode="api"):
        start_time = time.time()
        if mode == "api": result = self._generate_by_api(query)
        elif mode == "lora": result = self._generate_by_lora(query)
        else: result = "Invalid Mode"
        
        return {
            "query": query, "mode": mode, "keywords": result,
            "latency_ms": round((time.time() - start_time) * 1000, 2)
        }
