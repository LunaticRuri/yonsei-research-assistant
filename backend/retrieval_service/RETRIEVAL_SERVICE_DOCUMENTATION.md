# Retrieval Service ë¬¸ì„œ

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì„œë¹„ìŠ¤ êµ¬ì¡°](#ì„œë¹„ìŠ¤-êµ¬ì¡°)
3. [í•µì‹¬ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (SearchExecutor)](#í•µì‹¬-ê²€ìƒ‰-íŒŒì´í”„ë¼ì¸-searchexecutor)
4. [ì£¼ìš” ì»´í¬ë„ŒíŠ¸](#ì£¼ìš”-ì»´í¬ë„ŒíŠ¸)
5. [ë°ì´í„° ëª¨ë¸](#ë°ì´í„°-ëª¨ë¸)
6. [ì‚¬ìš© ì˜ˆì‹œ](#ì‚¬ìš©-ì˜ˆì‹œ)
7. [ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜](#ì„¤ì •-ë°-í™˜ê²½-ë³€ìˆ˜)

---

## ê°œìš”

**Retrieval Service**ëŠ” Yonsei Research Assistantì˜ í•µì‹¬ ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ, ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ í•™ìˆ  ìë£Œë¥¼ ê²€ìƒ‰í•˜ê³ , ì¬ìˆœìœ„í™”(reranking)í•˜ë©°, í’ˆì§ˆì„ í‰ê°€(CRAG)í•˜ëŠ” í†µí•© ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- **ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•© ê²€ìƒ‰**: ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì†Œì¥ìë£Œ, ì „ììë£Œ, êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë²¡í„° DB
- **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë™ì‹œ ê²€ìƒ‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
- **Reranking & Fusion**: Cross-encoder ê¸°ë°˜ ë¬¸ì„œ ì¬ìˆœìœ„í™” ë° ë‹¤ì–‘í•œ ìœµí•© ì „ëµ
- **CRAG (Corrective RAG)**: LLM ê¸°ë°˜ ë¬¸ì„œ í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§
- **ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨**: ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì´ ë‚®ì„ ê²½ìš° ìë™ ê°ì§€

### ì„œë¹„ìŠ¤ ì •ë³´
- **í¬íŠ¸**: 8003
- **ì—”ë“œí¬ì¸íŠ¸**: 
  - `POST /search`: ë©”ì¸ ê²€ìƒ‰ API
  - `GET /health`: ì„œë¹„ìŠ¤ ë° ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
  - `GET /`: ì„œë¹„ìŠ¤ ì •ë³´

---

## ì„œë¹„ìŠ¤ êµ¬ì¡°

```
retrieval_service/
â”œâ”€â”€ main.py                          # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
â”œâ”€â”€ config.py                        # ì„œë¹„ìŠ¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜, ëª¨ë¸ ì„¤ì •)
â”‚
â”œâ”€â”€ services/                        # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”œâ”€â”€ search_executor.py          # ğŸ”¥ ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì¡°ì •ì
â”‚   â”œâ”€â”€ retriever.py                # ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
â”‚   â”œâ”€â”€ ranker.py                   # Reranking & Fusion
â”‚   â””â”€â”€ refiner.py                  # CRAG í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§
â”‚
â”œâ”€â”€ adapters/                        # ë°ì´í„° ì†ŒìŠ¤ë³„ ì–´ëŒ‘í„°
â”‚   â”œâ”€â”€ base_adapters.py            # BaseRetriever ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ library_holdings_adapter.py # ì—°ì„¸ëŒ€ ì†Œì¥ìë£Œ ì–´ëŒ‘í„°
â”‚   â”œâ”€â”€ electronic_resources_adapter.py # ì—°ì„¸ëŒ€ ì „ììë£Œ ì–´ëŒ‘í„°
â”‚   â””â”€â”€ vectordb_adapter.py         # FAISS ë²¡í„° DB ì–´ëŒ‘í„°
â”‚
â”œâ”€â”€ scrapers/                        # ì›¹ ìŠ¤í¬ë˜í•‘ ì—”ì§„
â”‚   â”œâ”€â”€ base_scraper.py             # ê³µí†µ ìŠ¤í¬ë˜í•‘ ë¡œì§ (ë¡œê·¸ì¸, ì„¸ì…˜ ê´€ë¦¬)
â”‚   â”œâ”€â”€ library_holdings_scraper.py # ì†Œì¥ìë£Œ ìŠ¤í¬ë˜í¼
â”‚   â”œâ”€â”€ electronic_resources_scraper.py # ì „ììë£Œ ìŠ¤í¬ë˜í¼
â”‚   â””â”€â”€ search_params.py            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ëª¨ë¸
â”‚
â”œâ”€â”€ data/                            # ë²¡í„° DB ë°ì´í„°
â”‚   â”œâ”€â”€ faiss.index                 # FAISS ì¸ë±ìŠ¤
â”‚   â””â”€â”€ build_faiss_index.py        # ì¸ë±ìŠ¤ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_step1_retrieval.py     # ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
```

---

## í•µì‹¬ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (SearchExecutor)

`SearchExecutor`ëŠ” ì „ì²´ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. **5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

### íŒŒì´í”„ë¼ì¸ ê°œìš”

```python
class SearchExecutor:
    def __init__(self):
        self.retriever = RetrieverService()  # Step 1: ê²€ìƒ‰
        self.ranker = RankerService()        # Step 2: Reranking
        self.refiner = RefinerService()      # Step 3-5: CRAG í‰ê°€ ë° í•„í„°ë§
```

### ğŸ”¥ 5ë‹¨ê³„ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤

#### **Step 1: ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ (Retriever)**

```python
# ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰
raw_documents = await self.retriever.retrieve_all(request)
```

**ì—­í• **:
- Strategy Serviceë¡œë¶€í„° ë°›ì€ `SearchRequest`ë¥¼ ê° ì–´ëŒ‘í„°ì— ì „ë‹¬
- ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤(ì—°ì„¸ëŒ€ ì†Œì¥ìë£Œ, ì „ììë£Œ, ë²¡í„° DB)ì—ì„œ **ë³‘ë ¬ ê²€ìƒ‰**
- ê° ì–´ëŒ‘í„°ëŠ” `request_to_search_params()`ë¡œ ìš”ì²­ì„ ë³€í™˜ í›„ `search()` ì‹¤í–‰

**ì£¼ìš” íŠ¹ì§•**:
- `asyncio.gather()`ë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
- Vector DBëŠ” deadlock ë°©ì§€ë¥¼ ìœ„í•´ ë³„ë„ ì²˜ë¦¬
- ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ì†ŒìŠ¤ë§Œ ìŠ¤í‚µí•˜ê³  ë‹¤ë¥¸ ì†ŒìŠ¤ëŠ” ê³„ì† ì§„í–‰

**ì¶œë ¥**:
- `List[Document]`: ì¤‘ë³µ ì œê±°ë˜ì§€ ì•Šì€ ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

---

#### **Step 2: Reranking & Fusion (Ranker)**

```python
# Cross-encoderë¡œ ì¬ìˆœìœ„í™” ë° ìœµí•©
ranked_documents = self.ranker.rerank_and_fuse(
    documents=raw_documents,
    user_query=request.user_query
)
```

**ì—­í• **:
- **ì¤‘ë³µ ì œê±°**: Content ê¸°ë°˜ í•´ì‹±ìœ¼ë¡œ ë™ì¼ ë¬¸ì„œ ì œê±°
- **Cross-encoder Reranking**: ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê° ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ê¹Šì´ ìˆê²Œ ì¬í‰ê°€
- **Fusion ì „ëµ ì ìš©**: ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²°ê³¼ë¥¼ í†µí•©

**Fusion ì „ëµ ì˜µì…˜**:
1. **RRF (Reciprocal Rank Fusion)**: 
   ```python
   RRF_score = Î£ 1/(k + rank_i)  # k=60 (ê¸°ë³¸ê°’)
   ```
   - ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ìˆœìœ„ë¥¼ ì¡°í™”ë¡­ê²Œ ìœµí•©
   - ì–´ëŠ í•œ ì†ŒìŠ¤ì— í¸í–¥ë˜ì§€ ì•ŠìŒ

2. **Weighted Fusion**: 
   ```python
   weighted_score = cross_encoder_score Ã— source_weight
   ```
   - ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš© (ì˜ˆ: vector_db=0.6, library=0.4)

3. **Cross-encoder Only**: 
   - Cross-encoder ì ìˆ˜ë§Œ ì‚¬ìš©

**í˜„ì¬ ìƒíƒœ**:
- âš ï¸ `tmp_rerank_and_fuse()` ì‚¬ìš© ì¤‘ (ë¬´ì‘ìœ„ ì •ë ¬)
- TODO: Rerank ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ í›„ `rerank_and_fuse()`ë¡œ ì „í™˜

**ì¶œë ¥**:
- `List[RankedDocument]`: ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ (top_kê°œ, ê¸°ë³¸ 20ê°œ)

---

#### **Step 3: CRAG í’ˆì§ˆ í‰ê°€ (Refiner)**

```python
# LLMìœ¼ë¡œ ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€
crag_results = await self.refiner.evaluate_relevance(
    documents=ranked_documents,
    user_query=request.user_query
)
```

**ì—­í• **:
- **CRAG (Corrective RAG)**: LLM(Gemini 1.5 Flash)ì„ ì‚¬ìš©í•´ ê° ë¬¸ì„œë¥¼ í‰ê°€
- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œ ë‚´ìš©ì˜ ê´€ë ¨ì„±ì„ 3ë‹¨ê³„ë¡œ ë¶„ë¥˜

**ê´€ë ¨ì„± ë“±ê¸‰**:
```python
class RelevanceLevel(Enum):
    CORRECT = "correct"      # ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ - ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€
    AMBIGUOUS = "ambiguous"  # ë¶€ë¶„ì  ê´€ë ¨ - ì¶”ê°€ ì •ë³´ í•„ìš”
    INCORRECT = "incorrect"  # ë¬´ê´€ ë˜ëŠ” ì˜¤í•´ì˜ ì†Œì§€
```

**í‰ê°€ í”„ë¡¬í”„íŠ¸**:
```
ì§ˆë¬¸: {user_query}
ë¬¸ì„œ ë‚´ìš©: {document_content}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨:
- CORRECT: ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€, ê´€ë ¨ì„± ë†’ìŒ
- AMBIGUOUS: ë¶€ë¶„ì  ê´€ë ¨, ì¶”ê°€ ì •ë³´ í•„ìš”
- INCORRECT: ë¬´ê´€í•˜ê±°ë‚˜ ì˜¤í•´ ì†Œì§€

JSON ì‘ë‹µ:
{
  "relevance": "CORRECT" | "AMBIGUOUS" | "INCORRECT",
  "confidence": 0.0~1.0,
  "reason": "íŒë‹¨ ê·¼ê±°"
}
```

**ì¶œë ¥**:
- `List[CRAGResult]`: ê° ë¬¸ì„œì˜ í‰ê°€ ê²°ê³¼ (relevance, confidence, reason)

---

#### **Step 4: í’ˆì§ˆ í•„í„°ë§ (Refiner)**

```python
# CRAG ê²°ê³¼ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§
filtered_documents = self.refiner.filter_by_quality(crag_results)
```

**ì—­í• **:
- CRAG í‰ê°€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ í•„í„°ë§

**í•„í„°ë§ ê·œì¹™**:
```python
if relevance == CORRECT:
    âœ… í¬í•¨  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
elif relevance == AMBIGUOUS:
    if confidence >= CRAG_RELEVANCE_THRESHOLD:  # ê¸°ë³¸ 0.6
        âœ… í¬í•¨  # ì‹ ë¢°ë„ ì¶©ë¶„
    else:
        âŒ ì œì™¸  # ì‹ ë¢°ë„ ë‚®ìŒ
elif relevance == INCORRECT:
    âŒ ì œì™¸  # ë¬´ì¡°ê±´ ì œì™¸
```

**ì¶œë ¥**:
- `List[RankedDocument]`: í’ˆì§ˆ ê²€ì¦ëœ ìµœì¢… ë¬¸ì„œ

---

#### **Step 5: ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ (Refiner)**

```python
# INCORRECT ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
needs_web = self.refiner.needs_web_search(crag_results)
```

**ì—­í• **:
- ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì´ ë‚®ì„ ê²½ìš° ì™¸ë¶€ ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨

**íŒë‹¨ ê¸°ì¤€**:
```python
incorrect_ratio = incorrect_count / total_documents

if incorrect_ratio > CRAG_INCORRECT_RATIO_THRESHOLD:  # ê¸°ë³¸ 0.7
    needs_web_search = True  # 70% ì´ìƒì´ INCORRECTë©´ ì›¹ ê²€ìƒ‰
```

**ì¶œë ¥**:
- `bool`: ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€

---

### ìµœì¢… ê²°ê³¼ ë°˜í™˜

```python
return RetrievalResult(
    documents=filtered_documents,         # ìµœì¢… ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    crag_analysis=crag_results,          # ì „ì²´ CRAG í‰ê°€ ê²°ê³¼
    metadata={                            # ê²€ìƒ‰ í†µê³„
        'processing_time_seconds': elapsed_time,
        'total_retrieved': len(raw_documents),
        'after_rerank': len(ranked_documents),
        'after_crag': len(filtered_documents),
        'sources_used': [...]
    },
    needs_web_search=needs_web            # ì›¹ ê²€ìƒ‰ í•„ìš” í”Œë˜ê·¸
)
```

---

## ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. RetrieverService

**ìœ„ì¹˜**: `services/retriever.py`

**ì—­í• **: 
- Strategy Serviceì˜ ë¼ìš°íŒ… ê²°ì •ì„ ì‹¤í–‰
- ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰

**ì£¼ìš” ë©”ì„œë“œ**:

```python
async def retrieve_all(self, request: SearchRequest) -> List[Document]:
    """
    ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰
    
    Process:
    1. request.routesì—ì„œ ì§€ì •ëœ ì†ŒìŠ¤ë³„ ì–´ëŒ‘í„° ì„ íƒ
    2. ê° ì–´ëŒ‘í„°ì˜ request_to_search_params() í˜¸ì¶œ
    3. asyncio.gather()ë¡œ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
    4. Vector DBëŠ” ë³„ë„ ì²˜ë¦¬ (deadlock ë°©ì§€)
    5. ëª¨ë“  ê²°ê³¼ ë³‘í•© í›„ ë°˜í™˜
    """
```

**ì–´ëŒ‘í„° ë“±ë¡**:
```python
self.adapters: Dict[RetrievalRoute, BaseRetriever] = {
    RetrievalRoute.YONSEI_HOLDINGS: LibraryHoldingsAdapter(),
    RetrievalRoute.YONSEI_ELECTRONICS: ElectronicResourcesAdapter(),
    RetrievalRoute.VECTOR_DB: VectorDBAdapter()
}
```

**ë³‘ë ¬ ì²˜ë¦¬ ë¡œì§**:
```python
# Vector DB ì œì™¸í•œ ë‹¤ë¥¸ ì†ŒìŠ¤ë“¤ ë³‘ë ¬ ì‹¤í–‰
tasks = []
for route in request.routes:
    if route is not RetrievalRoute.VECTOR_DB:
        tasks.append(self._retrieve_process(adapter, request, route))

results = await asyncio.gather(*tasks, return_exceptions=True)

# Vector DBëŠ” ë³„ë„ë¡œ ìˆœì°¨ ì²˜ë¦¬
if RetrievalRoute.VECTOR_DB in request.routes:
    vector_docs = await vector_adapter.search(params, top_k)
    results.append(vector_docs)
```

---

### 2. RankerService

**ìœ„ì¹˜**: `services/ranker.py`

**ì—­í• **: 
- ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìœµí•©í•˜ê³  ì¬ìˆœìœ„í™”
- Cross-encoder ëª¨ë¸ë¡œ ì¿¼ë¦¬-ë¬¸ì„œ ê´€ë ¨ì„± ì¬í‰ê°€

**ì£¼ìš” ë©”ì„œë“œ**:

```python
def rerank_and_fuse(
    self, 
    documents: List[Document], 
    user_query: str,
    method: str = "rrf"
) -> List[RankedDocument]:
    """
    Rerank + Fusion íŒŒì´í”„ë¼ì¸
    
    Steps:
    1. _deduplicate(): Content ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    2. _cross_encoder_rerank(): Cross-encoderë¡œ ì¬ì ìˆ˜
    3. Fusion ì „ëµ ì ìš© (RRF/Weighted/Cross-encoder)
    4. Top-K í•„í„°ë§ ë° ìˆœìœ„ ë¶€ì—¬
    """
```

**Fusion ì „ëµë“¤**:

1. **RRF (Reciprocal Rank Fusion)**:
```python
def _reciprocal_rank_fusion(self, documents, k=60):
    """
    ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ì¡°í™”ë¡­ê²Œ ìœµí•©
    
    Formula: RRF_score = Î£ 1/(k + rank_i)
    
    Example:
    - Source A: rank 1 â†’ 1/(60+1) = 0.0164
    - Source B: rank 5 â†’ 1/(60+5) = 0.0154
    - Final: 0.0164 + 0.0154 = 0.0318
    """
```

2. **Weighted Fusion**:
```python
def _weighted_fusion(self, documents, weights=None):
    """
    ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
    
    Default weights:
    - vector_db: 0.6
    - yonsei_library: 0.4
    """
```

**í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ì‹œ ë©”ì„œë“œ**:
```python
def tmp_rerank_and_fuse(self, documents, user_query):
    """
    ì„ì‹œ Rerank + Fusion (ë‹¨ìˆœ ë¬´ì‘ìœ„ ì •ë ¬)
    TODO: Rerank ëª¨ë¸ íŒŒì¸íŠœë‹ í›„ ì œê±°
    """
```

---

### 3. RefinerService

**ìœ„ì¹˜**: `services/refiner.py`

**ì—­í• **: 
- CRAG (Corrective RAG) - ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
- ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§ ë° ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨

**ì£¼ìš” ë©”ì„œë“œ**:

```python
async def evaluate_relevance(
    self,
    documents: List[RankedDocument],
    user_query: str
) -> List[CRAGResult]:
    """
    ê° ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ CORRECT/AMBIGUOUS/INCORRECTë¡œ í‰ê°€
    
    Process:
    1. ê° ë¬¸ì„œì— ëŒ€í•´ _evaluate_single_document() í˜¸ì¶œ
    2. LLM(Gemini 1.5 Flash)ìœ¼ë¡œ ê´€ë ¨ì„± í‰ê°€
    3. JSON ì‘ë‹µ íŒŒì‹± â†’ CRAGResult ìƒì„±
    4. ì‹¤íŒ¨ ì‹œ AMBIGUOUSë¡œ í´ë°±
    """
```

```python
def filter_by_quality(
    self,
    crag_results: List[CRAGResult]
) -> List[RankedDocument]:
    """
    CRAG í‰ê°€ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§
    
    Rules:
    - CORRECT: ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - AMBIGUOUS: confidence >= threshold (0.6) ì´ë©´ í¬í•¨
    - INCORRECT: ì œê±°
    """
```

```python
def needs_web_search(self, crag_results: List[CRAGResult]) -> bool:
    """
    INCORRECT ë¹„ìœ¨ì´ 70% ì´ìƒì´ë©´ ì™¸ë¶€ ì›¹ ê²€ìƒ‰ í•„ìš”
    
    Formula:
    incorrect_ratio = incorrect_count / total_documents
    return incorrect_ratio > 0.7
    """
```

**LLM ì„¤ì •**:
```python
def __init__(self):
    genai.configure(api_key=settings.GEMINI_API_KEY)
    self.model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config={"response_mime_type": "application/json"}
    )
```

---

### 4. ì–´ëŒ‘í„° (Adapters)

ëª¨ë“  ì–´ëŒ‘í„°ëŠ” `BaseRetriever` ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

#### **BaseRetriever ì¸í„°í˜ì´ìŠ¤**

**ìœ„ì¹˜**: `adapters/base_adapters.py`

```python
class BaseRetriever(ABC):
    """ëª¨ë“  ê²€ìƒ‰ ì–´ëŒ‘í„°ê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    async def request_to_search_params(self, request: SearchRequest):
        """SearchRequestë¥¼ ì–´ëŒ‘í„°ë³„ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜"""
        pass

    @abstractmethod
    async def search(self, search_params, top_k: int) -> List[Document]:
        """í†µì¼ëœ ê²€ìƒ‰ ë©”ì„œë“œ"""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
        pass
    
    @property
    @abstractmethod
    def source_name(self) -> str:
        """ë°ì´í„° ì†ŒìŠ¤ ì‹ë³„ì"""
        pass
```

---

#### **LibraryHoldingsAdapter**

**ìœ„ì¹˜**: `adapters/library_holdings_adapter.py`

**ì—­í• **: ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì†Œì¥ìë£Œ(ë‹¨í–‰ë³¸, í•™ìœ„ë…¼ë¬¸ ë“±) ê²€ìƒ‰

**ì£¼ìš” ê¸°ëŠ¥**:
- `SearchRequest` â†’ `LibraryHoldingsSearchParams` ë³€í™˜
- `LibraryHoldingsScraper`ë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ìŠ¤í¬ë˜í•‘
- `LibraryHoldingInfo` â†’ `Document` í‘œì¤€í™”

**ì˜ˆì‹œ**:
```python
# SearchRequest ë³€í™˜
search_params = LibraryHoldingsSearchParams(
    query="ì¸ê³µì§€ëŠ¥",
    search_field=LibrarySearchField.TITLE,
    additional_queries=[
        {
            "search_field": LibrarySearchField.AUTHOR,
            "query": "ì´ì¤‘ì›",
            "operator": QueryOperator.AND
        }
    ],
    year_range={"from_year": 2020, "to_year": 2024},
    material_types=[HoldingsMaterialType.BOOK]
)

# ê²€ìƒ‰ ì‹¤í–‰
documents = await adapter.search(search_params, top_k=10)
```

**ë¬¸ì„œ í‘œì¤€í™”**:
```python
# LibraryHoldingInfo â†’ Document
Document(
    content=f"{holding.title}\n\n{holding.book_description}",
    metadata={
        "title": holding.title,
        "author": holding.author,
        "publication_year": holding.publication_year,
        "isbn": holding.isbn,
        "detail_url": holding.detail_url,
        "data_source": "yonsei_holdings"
    },
    score=1.0 / (rank + 1)  # ìˆœìœ„ ê¸°ë°˜ ìŠ¤ì½”ì–´
)
```

---

#### **ElectronicResourcesAdapter**

**ìœ„ì¹˜**: `adapters/electronic_resources_adapter.py`

**ì—­í• **: ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì „ììë£Œ(í•™ìˆ ë…¼ë¬¸, E-Book, ì €ë„ ë“±) ê²€ìƒ‰

**ì£¼ìš” ê¸°ëŠ¥**:
- `SearchRequest` â†’ `ElectronicResourcesSearchParams` ë³€í™˜
- `ElectronicResourcesScraper`ë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘
- `ElectronicResourceInfo` â†’ `Document` í‘œì¤€í™”

**í•„í„° ì§€ì›**:
- `year_range`: ì¶œíŒ ì—°ë„ ë²”ìœ„
- `academic_journals_only`: í•™ìˆ ì§€ ë…¼ë¬¸ë§Œ
- `foreign_language`: ì™¸êµ­ì–´ ìë£Œ í¬í•¨ ì—¬ë¶€

**ë¬¸ì„œ í‘œì¤€í™” ì˜ˆì‹œ**:
```python
Document(
    content=f"{resource.title}\n\n{resource.abstract}",
    metadata={
        "title": resource.title,
        "author": resource.author,
        "publication_year": resource.publication_year,
        "doi": resource.doi,
        "link_url": resource.link_url,
        "keywords": resource.keywords,
        "data_source": "yonsei_electronics"
    },
    score=1.0 / (rank + 1)
)
```

---

#### **VectorDBAdapter**

**ìœ„ì¹˜**: `adapters/vectordb_adapter.py`

**ì—­í• **: FAISS ê¸°ë°˜ êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë„ì„œ ë²¡í„° DB ê²€ìƒ‰

**ì£¼ìš” ê¸°ëŠ¥**:
- ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (KURE-v1 ëª¨ë¸)
- FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰
- ë©”íƒ€ë°ì´í„° ì¡°íšŒ (Pickle + SQLite)

**ì¿¼ë¦¬ ì²˜ë¦¬ ë¡œì§**:
```python
# AND ì—°ì‚°ì: ì¿¼ë¦¬ ê²°í•©
if operator_1 == QueryOperator.AND:
    query_1 += " " + query_2

# OR ì—°ì‚°ì: ë³„ë„ ë²¡í„° ìƒì„±
if operator_1 == QueryOperator.OR:
    vector_1 = encode(query_1)
    vector_2 = encode(query_2)
    # ê°ê° ê²€ìƒ‰ í›„ ê²°ê³¼ ë³‘í•©

# NOT ì—°ì‚°ì: ë¬´ì‹œ (ë²¡í„° ê²€ìƒ‰ì—ì„œ ì§€ì› ì–´ë ¤ì›€)
```

**ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤**:
```python
# 1. ì¿¼ë¦¬ ì„ë² ë”©
encoder = SentenceTransformer("nlpai-lab/KURE-v1")
vector = encoder.encode([query])

# 2. FAISS ê²€ìƒ‰
distances, indices = index.search(vector, top_k)

# 3. ë©”íƒ€ë°ì´í„° ì¡°íšŒ
metadata_id = metadata_faiss_map[faiss_id]
metadata = sqlite_db.query(metadata_id)

# 4. Document ìƒì„±
Document(
    content=metadata['title'] + "\n" + metadata['description'],
    metadata={
        "title": metadata['title'],
        "author": metadata['author'],
        "isbn": metadata['isbn'],
        "data_source": "vector_book_db"
    },
    score=1.0 / (1.0 + distance)
)
```

**ë°ì´í„° êµ¬ì¡°**:
- `faiss.index`: FAISS IVF ì¸ë±ìŠ¤ íŒŒì¼
- `id_to_metadata.pkl`: FAISS ID â†’ ë©”íƒ€ë°ì´í„° ID ë§¤í•‘
- `metadata.db`: SQLite ë©”íƒ€ë°ì´í„° DB

---

### 5. ìŠ¤í¬ë˜í¼ (Scrapers)

ëª¨ë“  ìŠ¤í¬ë˜í¼ëŠ” `BaseLibraryScraper`ë¥¼ ìƒì†í•©ë‹ˆë‹¤.

#### **BaseLibraryScraper**

**ìœ„ì¹˜**: `scrapers/base_scraper.py`

**ì—­í• **: 
- ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ë¡œê·¸ì¸ ì²˜ë¦¬
- ì„¸ì…˜ ê´€ë¦¬ (aiohttp + Playwright)
- ê³µí†µ ìŠ¤í¬ë˜í•‘ ìœ í‹¸ë¦¬í‹°

**ì£¼ìš” ê¸°ëŠ¥**:

```python
async def perform_login(self, user_id: str, user_pw: str) -> bool:
    """
    Playwrightë¡œ ë¸Œë¼ìš°ì € ìë™í™” ë¡œê·¸ì¸
    
    Process:
    1. Playwrightë¡œ headless ë¸Œë¼ìš°ì € ì‹¤í–‰
    2. ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™
    3. ì•„ì´ë””/ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
    4. ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
    5. ìƒì„±ëœ ì¿ í‚¤ë¥¼ aiohttp ì„¸ì…˜ìœ¼ë¡œ ë³µì‚¬
    """
```

**ë¡œê·¸ì¸ì´ í•„ìš”í•œ ì´ìœ **:
- ì „ììë£Œ ê²€ìƒ‰ì€ ë¡œê·¸ì¸ í•„ìš”
- ì†Œì¥ìë£Œ ê²€ìƒ‰ì€ ë¡œê·¸ì¸ ë¶ˆí•„ìš” (í•˜ì§€ë§Œ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹œ ìœ ë¦¬)

**ì„¸ì…˜ ê´€ë¦¬**:
```python
# aiohttp ì„¸ì…˜ ìƒì„±
self.session = aiohttp.ClientSession(headers=self.headers)

# Playwright ì¿ í‚¤ â†’ aiohttp ì„¸ì…˜
for cookie in playwright_cookies:
    self.session.cookie_jar.update_cookies({
        cookie['name']: cookie['value']
    })
```

---

#### **LibraryHoldingsScraper**

**ìœ„ì¹˜**: `scrapers/library_holdings_scraper.py`

**ì—­í• **: ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì†Œì¥ìë£Œ ìŠ¤í¬ë˜í•‘

**ì£¼ìš” ë©”ì„œë“œ**:
```python
async def search(self, params: LibraryHoldingsSearchParams) -> List[LibraryHoldingInfo]:
    """
    ì†Œì¥ìë£Œ ê²€ìƒ‰
    
    Process:
    1. ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ URL ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë³€í™˜
    2. ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìš”ì²­ (BeautifulSoup íŒŒì‹±)
    3. ê° ê²°ê³¼ í•­ëª©ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    4. ìƒì„¸ í˜ì´ì§€ ìš”ì²­ (ë³‘ë ¬ ì²˜ë¦¬)
    5. LibraryHoldingInfo ê°ì²´ ìƒì„±
    """
```

**ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ í•„ë“œ**:
- ì œëª© (title)
- ì €ì (author)
- ìë£Œ ìœ í˜• (material_type)
- ë°œí–‰ ì‚¬í•­ (publication_info)
- ISBN
- ì±… ì†Œê°œ (book_description)
- ìƒì„¸ URL (detail_url)

---

#### **ElectronicResourcesScraper**

**ìœ„ì¹˜**: `scrapers/electronic_resources_scraper.py`

**ì—­í• **: ì—°ì„¸ëŒ€í•™êµ ë„ì„œê´€ ì „ììë£Œ ìŠ¤í¬ë˜í•‘

**ì£¼ìš” ë©”ì„œë“œ**:
```python
async def search(self, params: ElectronicResourcesSearchParams) -> List[ElectronicResourceInfo]:
    """
    ì „ììë£Œ ê²€ìƒ‰
    
    Process:
    1. ë¡œê·¸ì¸ (í•„ìˆ˜)
    2. ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ API ìš”ì²­ìœ¼ë¡œ ë³€í™˜
    3. ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ íŒŒì‹±
    4. ê° ìë£Œì˜ ì´ˆë¡, DOI, ì›ë¬¸ ë§í¬ ì¶”ì¶œ
    5. ElectronicResourceInfo ê°ì²´ ìƒì„±
    """
```

**ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ í•„ë“œ**:
- ì œëª© (title)
- ì €ì (author)
- ì¶œì²˜ (source): ì €ë„ëª…, ê¶Œí˜¸, í˜ì´ì§€
- ì¶œíŒë…„ (publication_year)
- DOI
- ì›ë¬¸ ë§í¬ (link_url)
- ì´ˆë¡ (abstract)
- í‚¤ì›Œë“œ (keywords)

---

## ë°ì´í„° ëª¨ë¸

### ìš”ì²­ ëª¨ë¸

#### **SearchRequest**

Strategy Service â†’ Retrieval Service ìš”ì²­ ëª¨ë¸

```python
class SearchRequest(BaseModel):
    queries: SearchQueries              # ê²€ìƒ‰ ì¿¼ë¦¬ (ìµœëŒ€ 3ê°œ)
    routes: List[RetrievalRoute]       # ê²€ìƒ‰ ì†ŒìŠ¤ë“¤
    filters: Optional[Dict[str, Any]]  # í•„í„° ì¡°ê±´
    top_k: int = 10                    # ê° ì†ŒìŠ¤ë³„ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
    user_query: str                    # ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸ (CRAGìš©)
```

**ì˜ˆì‹œ**:
```python
SearchRequest(
    queries=SearchQueries(
        query_1="artificial intelligence",
        search_field_1=ElectronicSearchField.TITLE,
        operator_1=QueryOperator.AND,
        query_2="machine learning",
        search_field_2=ElectronicSearchField.TITLE
    ),
    routes=[
        RetrievalRoute.YONSEI_ELECTRONICS,
        RetrievalRoute.VECTOR_DB
    ],
    filters={
        "year_range": (2020, 2024),
        "academic_journals_only": True
    },
    top_k=10,
    user_query="AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì— ê´€í•œ ìµœê·¼ í•™ìˆ ë…¼ë¬¸"
)
```

---

#### **SearchQueries**

ë©€í‹° ì¿¼ë¦¬ ëª¨ë¸ (ìµœëŒ€ 3ê°œ ì¿¼ë¦¬)

```python
class SearchQueries(BaseModel):
    query_1: str                        # í•„ìˆ˜
    search_field_1: Union[LibrarySearchField, ElectronicSearchField]
    operator_1: Optional[QueryOperator] # AND/OR/NOT
    
    query_2: Optional[str]
    search_field_2: Optional[Union[LibrarySearchField, ElectronicSearchField]]
    operator_2: Optional[QueryOperator]
    
    query_3: Optional[str]
    search_field_3: Optional[Union[LibrarySearchField, ElectronicSearchField]]
```

**ê²€ì¦ ê·œì¹™**:
- ì¿¼ë¦¬ëŠ” ìˆœì°¨ì ìœ¼ë¡œë§Œ ì…ë ¥ ê°€ëŠ¥: (query_1), (query_1, query_2), (query_1, query_2, query_3)
- query_2ê°€ ìˆìœ¼ë©´ search_field_2 í•„ìˆ˜
- query_3ì´ ìˆìœ¼ë©´ operator_2, search_field_3 í•„ìˆ˜

---

### ì‘ë‹µ ëª¨ë¸

#### **RetrievalResult**

Retrieval Serviceì˜ ìµœì¢… ì‘ë‹µ

```python
class RetrievalResult(BaseModel):
    documents: List[RankedDocument]    # CRAG í•„í„°ë§ + Rerank ì™„ë£Œ ë¬¸ì„œ
    crag_analysis: List[CRAGResult]    # ì „ì²´ ë¬¸ì„œ CRAG í‰ê°€
    metadata: Dict[str, Any]           # ê²€ìƒ‰ í†µê³„
    needs_web_search: bool             # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
```

**ë©”íƒ€ë°ì´í„° ì˜ˆì‹œ**:
```python
{
    'processing_time_seconds': 2.34,
    'total_retrieved': 45,
    'after_rerank': 20,
    'after_crag': 12,
    'sources_used': ['yonsei_electronics', 'vector_book_db']
}
```

---

#### **Document**

ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ

```python
class Document(BaseModel):
    content: str                       # ë¬¸ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸
    metadata: Dict[str, Any]          # ì¶œì²˜, ì œëª©, ì €ì, URL ë“±
    score: float = 0.0                # ê²€ìƒ‰ ìœ ì‚¬ë„ ì ìˆ˜
    doc_id: Optional[str] = None      # ë¬¸ì„œ ê³ ìœ  ID
```

---

#### **RankedDocument**

Rerank í›„ ìµœì¢… ë¬¸ì„œ

```python
class RankedDocument(BaseModel):
    content: str
    metadata: Dict[str, Any]
    rerank_score: float               # Cross-encoder ì¬ì ìˆ˜
    original_score: float             # ì´ˆê¸° ê²€ìƒ‰ ì ìˆ˜
    source: str                       # ë°ì´í„° ì†ŒìŠ¤
    rank: int                         # ìµœì¢… ìˆœìœ„ (1ë¶€í„° ì‹œì‘)
```

---

#### **CRAGResult**

CRAG í’ˆì§ˆ í‰ê°€ ê²°ê³¼

```python
class CRAGResult(BaseModel):
    document: RankedDocument
    relevance: RelevanceLevel         # CORRECT/AMBIGUOUS/INCORRECT
    confidence: float                 # 0.0~1.0
    reason: Optional[str]             # íŒë‹¨ ê·¼ê±°
```

---

### Enum íƒ€ì…

#### **RetrievalRoute**
```python
class RetrievalRoute(str, Enum):
    VECTOR_DB = "vector_book_db"              # êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë„ì„œ ë²¡í„° DB
    YONSEI_HOLDINGS = "yonsei_holdings"       # ì—°ì„¸ëŒ€ ë„ì„œê´€ ì†Œì¥ìë£Œ
    YONSEI_ELECTRONICS = "yonsei_electronics" # ì—°ì„¸ëŒ€ ë„ì„œê´€ ì „ììë£Œ
```

#### **QueryOperator**
```python
class QueryOperator(str, Enum):
    AND = "and"  # í•„ìˆ˜
    OR = "or"    # ì„ íƒ
    NOT = "not"  # ì œì™¸
```

#### **LibrarySearchField**
```python
class LibrarySearchField(str, Enum):
    TOTAL = "TOTAL"     # ì „ì²´
    TITLE = "1"         # ì„œëª…(ì±…ì œëª©)
    AUTHOR = "2"        # ì €ì
    PUBLISHER = "3"     # ì¶œíŒì‚¬
    SUBJECT = "4"       # ì£¼ì œì–´
```

#### **ElectronicSearchField**
```python
class ElectronicSearchField(str, Enum):
    TOTAL = ""      # ì „ì²´
    KEYWORD = "TX"  # í‚¤ì›Œë“œ
    TITLE = "TI"    # ì œëª©
    AUTHOR = "AU"   # ì €ì
    SUBJECT = "SU"  # ì£¼ì œì–´
```

---

## ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ê²€ìƒ‰

```python
from shared.models import (
    SearchRequest, 
    SearchQueries,
    RetrievalRoute,
    LibrarySearchField
)
from retrieval_service.services.search_executor import SearchExecutor

# ê²€ìƒ‰ ìš”ì²­ ìƒì„±
request = SearchRequest(
    queries=SearchQueries(
        query_1="ì¸ê³µì§€ëŠ¥",
        search_field_1=LibrarySearchField.TITLE
    ),
    routes=[RetrievalRoute.YONSEI_HOLDINGS],
    filters={
        "year_range": (2020, 2024),
        "material_types": ["BOOK"]
    },
    top_k=10,
    user_query="ì¸ê³µì§€ëŠ¥ì— ê´€í•œ ìµœê·¼ ë„ì„œ"
)

# ê²€ìƒ‰ ì‹¤í–‰
executor = SearchExecutor()
result = await executor.execute(request)

# ê²°ê³¼ ì¶œë ¥
print(f"ì´ {len(result.documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")
for doc in result.documents[:3]:
    print(f"ì œëª©: {doc.metadata['title']}")
    print(f"ìˆœìœ„: {doc.rank}, ì ìˆ˜: {doc.rerank_score}")
```

---

### 2. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰

```python
request = SearchRequest(
    queries=SearchQueries(
        query_1="artificial intelligence",
        search_field_1=ElectronicSearchField.TITLE,
        operator_1=QueryOperator.AND,
        query_2="machine learning",
        search_field_2=ElectronicSearchField.TITLE,
        operator_2=QueryOperator.OR,
        query_3="deep learning",
        search_field_3=ElectronicSearchField.TITLE
    ),
    routes=[RetrievalRoute.YONSEI_ELECTRONICS],
    filters={
        "year_range": (2020, 2024),
        "academic_journals_only": True
    },
    top_k=10,
    user_query="AIì™€ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ë…¼ë¬¸"
)
```

**ì¿¼ë¦¬ í•´ì„**:
- (AI AND ML) OR ë”¥ëŸ¬ë‹
- ì œëª©ì— í•´ë‹¹ í‚¤ì›Œë“œ í¬í•¨
- 2020-2024ë…„ í•™ìˆ ë…¼ë¬¸ë§Œ

---

### 3. ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰

```python
request = SearchRequest(
    queries=SearchQueries(
        query_1="ìì—°ì–´ì²˜ë¦¬",
        search_field_1=LibrarySearchField.SUBJECT
    ),
    routes=[
        RetrievalRoute.YONSEI_HOLDINGS,      # ì†Œì¥ìë£Œ
        RetrievalRoute.YONSEI_ELECTRONICS,   # ì „ììë£Œ
        RetrievalRoute.VECTOR_DB             # ë²¡í„° DB
    ],
    top_k=5,  # ê° ì†ŒìŠ¤ë‹¹ 5ê°œì”©
    user_query="ìì—°ì–´ì²˜ë¦¬ ê´€ë ¨ ìë£Œ"
)

result = await executor.execute(request)

# ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ í™•ì¸
print(result.metadata['sources_used'])  # ['yonsei_holdings', 'yonsei_electronics', 'vector_book_db']
print(f"ì´ ê²€ìƒ‰: {result.metadata['total_retrieved']}")  # ì•½ 15ê°œ
print(f"ìµœì¢… ë°˜í™˜: {len(result.documents)}")  # CRAG í•„í„°ë§ í›„
```

---

### 4. CRAG ë¶„ì„ ê²°ê³¼ í™•ì¸

```python
result = await executor.execute(request)

# CRAG í†µê³„
correct = sum(1 for r in result.crag_analysis if r.relevance == "correct")
ambiguous = sum(1 for r in result.crag_analysis if r.relevance == "ambiguous")
incorrect = sum(1 for r in result.crag_analysis if r.relevance == "incorrect")

print(f"CORRECT: {correct}, AMBIGUOUS: {ambiguous}, INCORRECT: {incorrect}")

# ê° ë¬¸ì„œì˜ í‰ê°€ ê²°ê³¼
for crag in result.crag_analysis[:3]:
    print(f"\nì œëª©: {crag.document.metadata['title']}")
    print(f"ê´€ë ¨ì„±: {crag.relevance}")
    print(f"ì‹ ë¢°ë„: {crag.confidence:.2f}")
    print(f"ì´ìœ : {crag.reason}")

# ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
if result.needs_web_search:
    print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
```

---

### 5. API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ

```python
import requests

# FastAPI ì„œë²„ ì‹¤í–‰ (localhost:8003)
# POST /search
response = requests.post(
    "http://localhost:8003/search",
    json={
        "queries": {
            "query_1": "blockchain",
            "search_field_1": "TITLE"
        },
        "routes": ["yonsei_holdings"],
        "top_k": 10,
        "user_query": "ë¸”ë¡ì²´ì¸ ê´€ë ¨ ë„ì„œ"
    }
)

result = response.json()
print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(result['documents'])}ê°œ ë¬¸ì„œ")
```

---

### 6. Health Check

```python
# GET /health
response = requests.get("http://localhost:8003/health")
status = response.json()

print(f"ì„œë¹„ìŠ¤ ìƒíƒœ: {status['status']}")  # healthy / degraded
print("ì–´ëŒ‘í„° ìƒíƒœ:")
for adapter, is_healthy in status['adapters'].items():
    print(f"  - {adapter}: {'âœ…' if is_healthy else 'âŒ'}")
```

---

## ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜

### config.py

**ìœ„ì¹˜**: `retrieval_service/config.py`

```python
class Settings(BaseSettings):
    # ì—°ì„¸ëŒ€í•™êµ ê³„ì • (ìŠ¤í¬ë˜í•‘ìš©)
    YONSEI_ID: str
    YONSEI_PW: str

    # ì„œë¹„ìŠ¤ ê¸°ë³¸ ì •ë³´
    SERVICE_NAME: str = "retrieval-service"
    SERVICE_PORT: int = 8003
    
    # VectorDB (FAISS + SQLite) ì„¤ì •
    FAISS_INDEX_PATH: str
    FAISS_ID_TO_METADATA_PATH: str
    METADATA_DB_PATH: str

    # ì„ë² ë”© ëª¨ë¸
    VECTOR_EMBEDDING_MODEL: str = "nlpai-lab/KURE-v1"
    VECTOR_DIMENSION: int = 1024
    
    # Reranking ì„¤ì •
    RERANK_MODEL: str = "BAAI/bge-reranker-v2-m3"
    RERANK_TOP_K: int = 20
    FUSION_METHOD: str = "rrf"  # 'rrf' | 'weighted' | 'cross_encoder'
    
    # CRAG ì„¤ì •
    CRAG_LLM_MODEL: str = "gemini-1.5-flash"
    CRAG_RELEVANCE_THRESHOLD: float = 0.6
    CRAG_INCORRECT_RATIO_THRESHOLD: float = 0.7
    
    # API í‚¤
    GEMINI_API_KEY: str
    
    # ë¡œê¹…
    LOG_LEVEL: str = "INFO"
```

---

### .env íŒŒì¼

```bash
# ì—°ì„¸ëŒ€í•™êµ ê³„ì •
YONSEI_ID=your_student_id
YONSEI_PW=your_password

# VectorDB ê²½ë¡œ
FAISS_INDEX_PATH=/path/to/faiss.index
FAISS_ID_TO_METADATA_PATH=/path/to/id_to_metadata.pkl
METADATA_DB_PATH=/path/to/metadata.db

# Gemini API Key
GEMINI_API_KEY=your_gemini_api_key
```

---

## ì£¼ìš” íŠ¹ì§• ì •ë¦¬

### 1. **ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•©**
- ì—°ì„¸ëŒ€ ë„ì„œê´€ ì†Œì¥ìë£Œ
- ì—°ì„¸ëŒ€ ë„ì„œê´€ ì „ììë£Œ
- êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë²¡í„° DB
- í†µì¼ëœ `Document` ëª¨ë¸ë¡œ í‘œì¤€í™”

### 2. **ë³‘ë ¬ ì²˜ë¦¬**
- `asyncio.gather()`ë¡œ ì—¬ëŸ¬ ì†ŒìŠ¤ ë™ì‹œ ê²€ìƒ‰
- Vector DBëŠ” deadlock ë°©ì§€ë¥¼ ìœ„í•´ ë³„ë„ ì²˜ë¦¬
- ì„±ëŠ¥ ìµœì í™”

### 3. **ê³ ê¸‰ Reranking**
- Cross-encoder ëª¨ë¸ (BAAI/bge-reranker-v2-m3)
- RRF, Weighted, Cross-encoder Fusion ì§€ì›
- ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥

### 4. **CRAG í’ˆì§ˆ í‰ê°€**
- LLM(Gemini 1.5 Flash) ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€
- CORRECT/AMBIGUOUS/INCORRECT 3ë‹¨ê³„ ë¶„ë¥˜
- ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§

### 5. **ì›¹ ê²€ìƒ‰ íŠ¸ë¦¬ê±°**
- INCORRECT ë¹„ìœ¨ì´ 70% ì´ìƒì´ë©´ ì›¹ ê²€ìƒ‰ í•„ìš” ì‹ í˜¸
- ë‚´ë¶€ ê²€ìƒ‰ í’ˆì§ˆ ìë™ ëª¨ë‹ˆí„°ë§

### 6. **ìœ ì—°í•œ ì¿¼ë¦¬ êµ¬ì„±**
- ìµœëŒ€ 3ê°œ ì¿¼ë¦¬ ì¡°í•©
- AND/OR/NOT ì—°ì‚°ì ì§€ì›
- ì†ŒìŠ¤ë³„ ê²€ìƒ‰ í•„ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 7. **ìƒì„¸í•œ ë©”íƒ€ë°ì´í„°**
- ì²˜ë¦¬ ì‹œê°„, ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ ë“± í†µê³„
- CRAG ë¶„ì„ ê²°ê³¼ í¬í•¨
- ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ ìš©ì´

---

## ì œí•œ ì‚¬í•­ ë° TODO

### í˜„ì¬ ì œí•œ ì‚¬í•­

1. **Reranking ë¯¸ì™„ì„±**
   - í˜„ì¬: `tmp_rerank_and_fuse()` ì‚¬ìš© (ë¬´ì‘ìœ„ ì •ë ¬)
   - TODO: Rerank ëª¨ë¸ íŒŒì¸íŠœë‹ í›„ `rerank_and_fuse()`ë¡œ ì „í™˜

2. **Vector DB NOT ì—°ì‚°ì ë¯¸ì§€ì›**
   - Vector ê²€ìƒ‰ íŠ¹ì„±ìƒ NOT ì—°ì‚°ì êµ¬í˜„ ì–´ë ¤ì›€
   - í˜„ì¬: NOT ì—°ì‚°ìëŠ” ë¬´ì‹œë¨

3. **ì¤‘ë³µ ì œê±° ì•Œê³ ë¦¬ì¦˜**
   - í˜„ì¬: Content í•´ì‹± (ë‹¨ìˆœ)
   - TODO: Embedding ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

4. **ë¡œê·¸ì¸ ì¿ í‚¤ ê´€ë¦¬**
   - ì„¸ì…˜ ë§Œë£Œ ì‹œ ì¬ë¡œê·¸ì¸ ë¡œì§ í•„ìš”
   - í˜„ì¬: ìˆ˜ë™ìœ¼ë¡œ ì¬ì‹œì‘ í•„ìš”

### í–¥í›„ ê°œì„  ì‚¬í•­

- [ ] Rerank ëª¨ë¸ íŒŒì¸íŠœë‹
- [ ] Embedding ê¸°ë°˜ ì¤‘ë³µ ì œê±°
- [ ] CRAG í”„ë¡¬í”„íŠ¸ ìµœì í™”
- [ ] ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
- [ ] ê²€ìƒ‰ ë¡œê·¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
- [ ] ì›¹ ê²€ìƒ‰ í†µí•© (Tavily, Serper ë“±)

---

## í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸: `tests/test_step1_retrieval.py`

```bash
# ì „ììë£Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
cd backend
python3 -m retrieval_service.tests.test_step1_retrieval --test electronic

# ì†Œì¥ìë£Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
python3 -m retrieval_service.tests.test_step1_retrieval --test holdings

# ë²¡í„° DB ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
python3 -m retrieval_service.tests.test_step1_retrieval --test vector

# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python3 -m retrieval_service.tests.test_step1_retrieval --test all
```

---

## ì°¸ê³  ìë£Œ

- **CRAG ë…¼ë¬¸**: Corrective Retrieval Augmented Generation
- **RRF ë…¼ë¬¸**: Reciprocal Rank Fusion
- **Cross-encoder ëª¨ë¸**: BAAI/bge-reranker-v2-m3
- **ì„ë² ë”© ëª¨ë¸**: nlpai-lab/KURE-v1 (í•œêµ­ì–´ íŠ¹í™”)
- **LLM**: Gemini 1.5 Flash (Google)

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-11-22  
**ë²„ì „**: 1.0.0
